[[0.196665,{"lines":{"0":[["                                                                                                     ",{}]],"1":[["                                                                                                     ",{}]],"2":[["                                                                                                     ",{}]],"3":[["                                                                                                     ",{}]],"4":[["                                                                                                     ",{}]],"5":[["                                                                                                     ",{}]],"6":[["                                                                                                     ",{}]],"7":[["                                                                                                     ",{}]],"8":[["                                                                                                     ",{}]],"9":[["                                                                                                     ",{}]],"10":[["                                                                                                     ",{}]],"11":[["                                                                                                     ",{}]],"12":[["                                                                                                     ",{}]],"13":[["                                                                                                     ",{}]],"14":[["                                                                                                     ",{}]],"15":[["                                                                                                     ",{}]],"16":[["                                                                                                     ",{}]],"17":[["                                                                                                     ",{}]],"18":[["                                                                                                     ",{}]],"19":[["                                                                                                     ",{}]],"20":[["                                                                                                     ",{}]],"21":[["                                                                                                     ",{}]],"22":[["                                                                                                     ",{}]],"23":[["                                                                                                     ",{}]],"24":[["                                                                                                     ",{}]],"25":[["                                                                                                     ",{}]],"26":[["                                                                                                     ",{}]],"27":[["                                                                                                     ",{}]],"28":[["                                                                                                     ",{}]],"29":[["                                                                                                     ",{}]],"30":[["                                                                                                     ",{}]],"31":[["                                                                                                     ",{}]],"32":[["                                                                                                     ",{}]],"33":[["                                                                                                     ",{}]],"34":[["                                                                                                     ",{}]],"35":[["                                                                                                     ",{}]],"36":[["                                                                                                     ",{}]],"37":[["                                                                                                     ",{}]],"38":[["                                                                                                     ",{}]],"39":[["                                                                                                     ",{}]],"40":[["                                                                                                     ",{}]],"41":[["                                                                                                     ",{}]]},"cursor":{"x":0,"y":0,"visible":true}}],[0.073381,{"lines":{"0":[["╭─",{}],["zzl",{"fg":40}],[" ",{}],["at",{"fg":239}],[" ",{}],["zhus-MacBook-Air-3",{"fg":33}],[" ",{}],["in",{"fg":239}],[" ",{}],["~/projects/scala",{"fg":226,"bold":true}],[" ",{}],["using",{"fg":239}],[" ‹›",{"fg":243}],["                                             ",{}]],"1":[["╰─○                                                                                                  ",{}]]},"cursor":{"x":4,"y":1}}],[0.990622,{"lines":{"1":[["╰─○ p                                                                                                ",{}]]},"cursor":{"x":5}}],[0.221214,{"lines":{"1":[["╰─○ py                                                                                               ",{}]]},"cursor":{"x":6}}],[0.258156,{"lines":{"1":[["╰─○ pys                                                                                              ",{}]]},"cursor":{"x":7}}],[0.290727,{"lines":{"1":[["╰─○ pyspark                                                                                          ",{}]]},"cursor":{"x":11}}],[0.875939,{"cursor":{"x":0,"y":2}}],[0.015324,{"lines":{"2":[["/Users/zzl/anaconda/bin/python                                                                       ",{}]]},"cursor":{"y":3}}],[0.026424,{"lines":{"3":[["Python 2.7.3 |Anaconda 1.4.0 (x86_64)| (default, Feb 25 2013, 18:45:56)                              ",{}]],"4":[["[GCC 4.0.1 (Apple Inc. build 5493)] on darwin                                                        ",{}]],"5":[["Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.                               ",{}]]},"cursor":{"y":6}}],[0.100487,{}],[1.198226,{"lines":{"6":[["14/04/13 17:04:59 INFO Slf4jEventHandler: Slf4jEventHandler started                                  ",{}]]},"cursor":{"y":7}}],[0.173902,{"lines":{"7":[["14/04/13 17:04:59 INFO SparkEnv: Registering BlockManagerMaster                                      ",{}]]},"cursor":{"y":8}}],[0.047873,{"lines":{"8":[["14/04/13 17:04:59 INFO DiskBlockManager: Created local directory at /var/folders/_g/0qtqr8zs2jx7n86sg",{}]],"9":[["xw3r_nw0000gn/T/spark-local-20140413170459-4c04                                                      ",{}]]},"cursor":{"y":10}}],[0.005234,{"lines":{"10":[["14/04/13 17:04:59 INFO MemoryStore: MemoryStore started with capacity 333.7 MB.                      ",{}]]},"cursor":{"y":11}}],[0.025933,{"lines":{"11":[["14/04/13 17:04:59 INFO ConnectionManager: Bound socket to port 63139 with id = ConnectionManagerId(19",{}]],"12":[["2.168.1.102,63139)                                                                                   ",{}]]},"cursor":{"y":13}}],[0.009457,{"lines":{"13":[["14/04/13 17:04:59 INFO BlockManagerMaster: Trying to register BlockManager                           ",{}]],"14":[["14/04/13 17:04:59 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 192.168.1.",{}]],"15":[["102:63139 with 333.7 MB RAM                                                                          ",{}]],"16":[["14/04/13 17:04:59 INFO BlockManagerMaster: Registered BlockManager                                   ",{}]]},"cursor":{"y":17}}],[0.132064,{"lines":{"17":[["14/04/13 17:04:59 INFO HttpBroadcast: Broadcast server started at http://192.168.1.102:63140         ",{}]]},"cursor":{"y":18}}],[0.009502,{"lines":{"18":[["14/04/13 17:04:59 INFO SparkEnv: Registering MapOutputTracker                                        ",{}]],"19":[["14/04/13 17:04:59 INFO HttpFileServer: HTTP File server directory is /var/folders/_g/0qtqr8zs2jx7n86s",{}]],"20":[["gxw3r_nw0000gn/T/spark-80399ca2-19cc-4757-a712-bd7d80396cc5                                          ",{}]]},"cursor":{"y":21}}],[0.205381,{"lines":{"21":[["14/04/13 17:04:59 INFO SparkUI: Started Spark Web UI at http://192.168.1.102:4040                    ",{}]]},"cursor":{"y":22}}],[0.211952,{"lines":{"22":[["2014-04-13 17:05:00.160 java[7373:c103] Unable to load realm info from SCDynamicStore                ",{}]]},"cursor":{"y":23}}],[0.14657,{"lines":{"23":[["Welcome to                                                                                           ",{}]],"24":[["      ____              __                                                                           ",{}]],"25":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"26":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"27":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"28":[["      /_/                                                                                            ",{}]]},"cursor":{"y":30}}],[0.003236,{"lines":{"30":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"31":[["Spark context avaiable as sc.                                                                        ",{}]],"32":[["h[1] >>> ",{"fg":2}],["                                                                                            ",{}]]},"cursor":{"x":9,"y":32}}],[0.618425,{"lines":{"32":[["h[1] >>> ",{"fg":2}],["s                                                                                           ",{}]]},"cursor":{"x":10}}],[0.252468,{"lines":{"32":[["h[1] >>> ",{"fg":2}],["sc                                                                                          ",{}]]},"cursor":{"x":11}}],[0.46489,{"lines":{"32":[["h[1] >>> ",{"fg":2}],["sc.                                                                                         ",{}]]},"cursor":{"x":12}}],[0.374229,{}],[0.869596,{"lines":{"33":[["sc.__class__(                   sc.__weakref__                  sc.addFile(                          ",{}]],"34":[["sc.__del__(                     sc._accumulatorServer           sc.addPyFile(                        ",{}]],"35":[["sc.__delattr__(                 sc._active_spark_context        sc.batchSize                         ",{}]]},"cursor":{"x":0,"y":36}}],[0.000276,{"lines":{"0":[["14/04/13 17:04:59 INFO MemoryStore: MemoryStore started with capacity 333.7 MB.                      ",{}]],"1":[["14/04/13 17:04:59 INFO ConnectionManager: Bound socket to port 63139 with id = ConnectionManagerId(19",{}]],"2":[["2.168.1.102,63139)                                                                                   ",{}]],"3":[["14/04/13 17:04:59 INFO BlockManagerMaster: Trying to register BlockManager                           ",{}]],"4":[["14/04/13 17:04:59 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 192.168.1.",{}]],"5":[["102:63139 with 333.7 MB RAM                                                                          ",{}]],"6":[["14/04/13 17:04:59 INFO BlockManagerMaster: Registered BlockManager                                   ",{}]],"7":[["14/04/13 17:04:59 INFO HttpBroadcast: Broadcast server started at http://192.168.1.102:63140         ",{}]],"8":[["14/04/13 17:04:59 INFO SparkEnv: Registering MapOutputTracker                                        ",{}]],"9":[["14/04/13 17:04:59 INFO HttpFileServer: HTTP File server directory is /var/folders/_g/0qtqr8zs2jx7n86s",{}]],"10":[["gxw3r_nw0000gn/T/spark-80399ca2-19cc-4757-a712-bd7d80396cc5                                          ",{}]],"11":[["14/04/13 17:04:59 INFO SparkUI: Started Spark Web UI at http://192.168.1.102:4040                    ",{}]],"12":[["2014-04-13 17:05:00.160 java[7373:c103] Unable to load realm info from SCDynamicStore                ",{}]],"13":[["Welcome to                                                                                           ",{}]],"14":[["      ____              __                                                                           ",{}]],"15":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"16":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"17":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"18":[["      /_/                                                                                            ",{}]],"19":[["                                                                                                     ",{}]],"20":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"21":[["Spark context avaiable as sc.                                                                        ",{}]],"22":[["h[1] >>> ",{"fg":2}],["sc.                                                                                         ",{}]],"23":[["sc.__class__(                   sc.__weakref__                  sc.addFile(                          ",{}]],"24":[["sc.__del__(                     sc._accumulatorServer           sc.addPyFile(                        ",{}]],"25":[["sc.__delattr__(                 sc._active_spark_context        sc.batchSize                         ",{}]],"26":[["sc.__dict__                     sc._checkpointFile(             sc.broadcast(                        ",{}]],"27":[["sc.__doc__                      sc._ensure_initialized(         sc.clearFiles(                       ",{}]],"28":[["sc.__format__(                  sc._gateway(                    sc.defaultParallelism                ",{}]],"29":[["sc.__getattribute__(            sc._getJavaStorageLevel(        sc.environment                       ",{}]],"30":[["sc.__hash__(                    sc._javaAccumulator(            sc.jobName                           ",{}]],"31":[["sc.__init__(                    sc._jsc(                        sc.master                            ",{}]],"32":[["sc.__module__                   sc._jvm(                        sc.parallelize(                      ",{}]],"33":[["sc.__new__(                     sc._lock                        sc.pythonExec                        ",{}]],"34":[["sc.__reduce__(                  sc._next_accum_id               sc.setCheckpointDir(                 ",{}]],"35":[["sc.__reduce_ex__(               sc._pickled_broadcast_vars      sc.setSystemProperty(                ",{}]],"36":[["sc.__repr__(                    sc._python_includes             sc.sparkHome                         ",{}]],"37":[["sc.__setattr__(                 sc._takePartition(              sc.stop(                             ",{}]],"38":[["sc.__sizeof__(                  sc._temp_dir                    sc.textFile(                         ",{}]],"39":[["sc.__str__(                     sc._writeIteratorToPickleFile(  sc.union(                            ",{}]],"40":[["sc.__subclasshook__(            sc.accumulator(                                                      ",{}]],"41":[["h[1] >>> ",{"fg":2}],["sc.                                                                                         ",{}]]},"cursor":{"x":12,"y":41}}],[2.664774,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["                                                                                            ",{}]]},"cursor":{"x":9}}],[1.538372,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["n                                                                                           ",{}]]},"cursor":{"x":10}}],[0.199577,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["nu                                                                                          ",{}]]},"cursor":{"x":11}}],[0.208099,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["num                                                                                         ",{}]]},"cursor":{"x":12}}],[0.322568,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["nums                                                                                        ",{}]]},"cursor":{"x":13}}],[0.36605,{"cursor":{"x":14}}],[0.120027,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["nums =                                                                                      ",{}]]},"cursor":{"x":15}}],[0.104605,{"cursor":{"x":16}}],[0.296438,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["nums = r                                                                                    ",{}]]},"cursor":{"x":17}}],[0.102562,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["nums = ra                                                                                   ",{}]]},"cursor":{"x":18}}],[0.11164,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["nums = ran                                                                                  ",{}]]},"cursor":{"x":19}}],[0.095807,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["nums = rang                                                                                 ",{}]]},"cursor":{"x":20}}],[0.099249,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["nums = range                                                                                ",{}]]},"cursor":{"x":21}}],[0.5657180000000001,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["nums = range(                                                                               ",{}]]},"cursor":{"x":22}}],[0.319792,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["nums = range(1                                                                              ",{}]]},"cursor":{"x":23}}],[0.24009,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["nums = range(10                                                                             ",{}]]},"cursor":{"x":24}}],[0.311753,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]]},"cursor":{"x":25}}],[0.48876,{"lines":{"0":[["14/04/13 17:04:59 INFO ConnectionManager: Bound socket to port 63139 with id = ConnectionManagerId(19",{}]],"1":[["2.168.1.102,63139)                                                                                   ",{}]],"2":[["14/04/13 17:04:59 INFO BlockManagerMaster: Trying to register BlockManager                           ",{}]],"3":[["14/04/13 17:04:59 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 192.168.1.",{}]],"4":[["102:63139 with 333.7 MB RAM                                                                          ",{}]],"5":[["14/04/13 17:04:59 INFO BlockManagerMaster: Registered BlockManager                                   ",{}]],"6":[["14/04/13 17:04:59 INFO HttpBroadcast: Broadcast server started at http://192.168.1.102:63140         ",{}]],"7":[["14/04/13 17:04:59 INFO SparkEnv: Registering MapOutputTracker                                        ",{}]],"8":[["14/04/13 17:04:59 INFO HttpFileServer: HTTP File server directory is /var/folders/_g/0qtqr8zs2jx7n86s",{}]],"9":[["gxw3r_nw0000gn/T/spark-80399ca2-19cc-4757-a712-bd7d80396cc5                                          ",{}]],"10":[["14/04/13 17:04:59 INFO SparkUI: Started Spark Web UI at http://192.168.1.102:4040                    ",{}]],"11":[["2014-04-13 17:05:00.160 java[7373:c103] Unable to load realm info from SCDynamicStore                ",{}]],"12":[["Welcome to                                                                                           ",{}]],"13":[["      ____              __                                                                           ",{}]],"14":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"15":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"16":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"17":[["      /_/                                                                                            ",{}]],"18":[["                                                                                                     ",{}]],"19":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"20":[["Spark context avaiable as sc.                                                                        ",{}]],"21":[["h[1] >>> ",{"fg":2}],["sc.                                                                                         ",{}]],"22":[["sc.__class__(                   sc.__weakref__                  sc.addFile(                          ",{}]],"23":[["sc.__del__(                     sc._accumulatorServer           sc.addPyFile(                        ",{}]],"24":[["sc.__delattr__(                 sc._active_spark_context        sc.batchSize                         ",{}]],"25":[["sc.__dict__                     sc._checkpointFile(             sc.broadcast(                        ",{}]],"26":[["sc.__doc__                      sc._ensure_initialized(         sc.clearFiles(                       ",{}]],"27":[["sc.__format__(                  sc._gateway(                    sc.defaultParallelism                ",{}]],"28":[["sc.__getattribute__(            sc._getJavaStorageLevel(        sc.environment                       ",{}]],"29":[["sc.__hash__(                    sc._javaAccumulator(            sc.jobName                           ",{}]],"30":[["sc.__init__(                    sc._jsc(                        sc.master                            ",{}]],"31":[["sc.__module__                   sc._jvm(                        sc.parallelize(                      ",{}]],"32":[["sc.__new__(                     sc._lock                        sc.pythonExec                        ",{}]],"33":[["sc.__reduce__(                  sc._next_accum_id               sc.setCheckpointDir(                 ",{}]],"34":[["sc.__reduce_ex__(               sc._pickled_broadcast_vars      sc.setSystemProperty(                ",{}]],"35":[["sc.__repr__(                    sc._python_includes             sc.sparkHome                         ",{}]],"36":[["sc.__setattr__(                 sc._takePartition(              sc.stop(                             ",{}]],"37":[["sc.__sizeof__(                  sc._temp_dir                    sc.textFile(                         ",{}]],"38":[["sc.__str__(                     sc._writeIteratorToPickleFile(  sc.union(                            ",{}]],"39":[["sc.__subclasshook__(            sc.accumulator(                                                      ",{}]],"40":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"41":[["                                                                                                     ",{}]]},"cursor":{"x":0}}],[0.000217,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["                                                                                            ",{}]]},"cursor":{"x":9}}],[2.230785,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["r                                                                                           ",{}]]},"cursor":{"x":10}}],[0.223723,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rd                                                                                          ",{}]]},"cursor":{"x":11}}],[0.15228,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd                                                                                         ",{}]]},"cursor":{"x":12}}],[0.353552,{"cursor":{"x":13}}],[0.104916,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd =                                                                                       ",{}]]},"cursor":{"x":14}}],[0.111155,{"cursor":{"x":15}}],[0.365941,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd = s                                                                                     ",{}]]},"cursor":{"x":16}}],[0.224266,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd = sc                                                                                    ",{}]]},"cursor":{"x":17}}],[1.075353,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd = sc.                                                                                   ",{}]]},"cursor":{"x":18}}],[0.35801,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd = sc.p                                                                                  ",{}]]},"cursor":{"x":19}}],[1.046499,{}],[0.776207,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd = sc.pa                                                                                 ",{}]]},"cursor":{"x":20}}],[0.288451,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(                                                                       ",{}]]},"cursor":{"x":30}}],[2.000293,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(n                                                                      ",{}]]},"cursor":{"x":31}}],[0.223151,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nu                                                                     ",{}]]},"cursor":{"x":32}}],[0.239755,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(num                                                                    ",{}]]},"cursor":{"x":33}}],[0.194295,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums                                                                   ",{}]]},"cursor":{"x":34}}],[0.518528,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]]},"cursor":{"x":35}}],[0.929917,{"lines":{"0":[["2.168.1.102,63139)                                                                                   ",{}]],"1":[["14/04/13 17:04:59 INFO BlockManagerMaster: Trying to register BlockManager                           ",{}]],"2":[["14/04/13 17:04:59 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 192.168.1.",{}]],"3":[["102:63139 with 333.7 MB RAM                                                                          ",{}]],"4":[["14/04/13 17:04:59 INFO BlockManagerMaster: Registered BlockManager                                   ",{}]],"5":[["14/04/13 17:04:59 INFO HttpBroadcast: Broadcast server started at http://192.168.1.102:63140         ",{}]],"6":[["14/04/13 17:04:59 INFO SparkEnv: Registering MapOutputTracker                                        ",{}]],"7":[["14/04/13 17:04:59 INFO HttpFileServer: HTTP File server directory is /var/folders/_g/0qtqr8zs2jx7n86s",{}]],"8":[["gxw3r_nw0000gn/T/spark-80399ca2-19cc-4757-a712-bd7d80396cc5                                          ",{}]],"9":[["14/04/13 17:04:59 INFO SparkUI: Started Spark Web UI at http://192.168.1.102:4040                    ",{}]],"10":[["2014-04-13 17:05:00.160 java[7373:c103] Unable to load realm info from SCDynamicStore                ",{}]],"11":[["Welcome to                                                                                           ",{}]],"12":[["      ____              __                                                                           ",{}]],"13":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"14":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"15":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"16":[["      /_/                                                                                            ",{}]],"17":[["                                                                                                     ",{}]],"18":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"19":[["Spark context avaiable as sc.                                                                        ",{}]],"20":[["h[1] >>> ",{"fg":2}],["sc.                                                                                         ",{}]],"21":[["sc.__class__(                   sc.__weakref__                  sc.addFile(                          ",{}]],"22":[["sc.__del__(                     sc._accumulatorServer           sc.addPyFile(                        ",{}]],"23":[["sc.__delattr__(                 sc._active_spark_context        sc.batchSize                         ",{}]],"24":[["sc.__dict__                     sc._checkpointFile(             sc.broadcast(                        ",{}]],"25":[["sc.__doc__                      sc._ensure_initialized(         sc.clearFiles(                       ",{}]],"26":[["sc.__format__(                  sc._gateway(                    sc.defaultParallelism                ",{}]],"27":[["sc.__getattribute__(            sc._getJavaStorageLevel(        sc.environment                       ",{}]],"28":[["sc.__hash__(                    sc._javaAccumulator(            sc.jobName                           ",{}]],"29":[["sc.__init__(                    sc._jsc(                        sc.master                            ",{}]],"30":[["sc.__module__                   sc._jvm(                        sc.parallelize(                      ",{}]],"31":[["sc.__new__(                     sc._lock                        sc.pythonExec                        ",{}]],"32":[["sc.__reduce__(                  sc._next_accum_id               sc.setCheckpointDir(                 ",{}]],"33":[["sc.__reduce_ex__(               sc._pickled_broadcast_vars      sc.setSystemProperty(                ",{}]],"34":[["sc.__repr__(                    sc._python_includes             sc.sparkHome                         ",{}]],"35":[["sc.__setattr__(                 sc._takePartition(              sc.stop(                             ",{}]],"36":[["sc.__sizeof__(                  sc._temp_dir                    sc.textFile(                         ",{}]],"37":[["sc.__str__(                     sc._writeIteratorToPickleFile(  sc.union(                            ",{}]],"38":[["sc.__subclasshook__(            sc.accumulator(                                                      ",{}]],"39":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"40":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"41":[["                                                                                                     ",{}]]},"cursor":{"x":0}}],[0.757078,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["                                                                                            ",{}]]},"cursor":{"x":9}}],[0.832252,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["r                                                                                           ",{}]]},"cursor":{"x":10}}],[0.361272,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rd                                                                                          ",{}]]},"cursor":{"x":11}}],[0.150868,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd                                                                                         ",{}]]},"cursor":{"x":12}}],[0.304449,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd.                                                                                        ",{}]]},"cursor":{"x":13}}],[0.655618,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd.c                                                                                       ",{}]]},"cursor":{"x":14}}],[0.705658,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd.co                                                                                      ",{}]]},"cursor":{"x":15}}],[0.430521,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd.col                                                                                     ",{}]]},"cursor":{"x":16}}],[0.584131,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd.coll                                                                                    ",{}]]},"cursor":{"x":17}}],[0.354735,{}],[4.1e-05,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd.collect                                                                                 ",{}]]},"cursor":{"x":20}}],[1.167027,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd.collect(                                                                                ",{}]]},"cursor":{"x":21}}],[0.054064,{"lines":{"41":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]]},"cursor":{"x":22}}],[0.511443,{"lines":{"0":[["14/04/13 17:04:59 INFO BlockManagerMaster: Trying to register BlockManager                           ",{}]],"1":[["14/04/13 17:04:59 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 192.168.1.",{}]],"2":[["102:63139 with 333.7 MB RAM                                                                          ",{}]],"3":[["14/04/13 17:04:59 INFO BlockManagerMaster: Registered BlockManager                                   ",{}]],"4":[["14/04/13 17:04:59 INFO HttpBroadcast: Broadcast server started at http://192.168.1.102:63140         ",{}]],"5":[["14/04/13 17:04:59 INFO SparkEnv: Registering MapOutputTracker                                        ",{}]],"6":[["14/04/13 17:04:59 INFO HttpFileServer: HTTP File server directory is /var/folders/_g/0qtqr8zs2jx7n86s",{}]],"7":[["gxw3r_nw0000gn/T/spark-80399ca2-19cc-4757-a712-bd7d80396cc5                                          ",{}]],"8":[["14/04/13 17:04:59 INFO SparkUI: Started Spark Web UI at http://192.168.1.102:4040                    ",{}]],"9":[["2014-04-13 17:05:00.160 java[7373:c103] Unable to load realm info from SCDynamicStore                ",{}]],"10":[["Welcome to                                                                                           ",{}]],"11":[["      ____              __                                                                           ",{}]],"12":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"13":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"14":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"15":[["      /_/                                                                                            ",{}]],"16":[["                                                                                                     ",{}]],"17":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"18":[["Spark context avaiable as sc.                                                                        ",{}]],"19":[["h[1] >>> ",{"fg":2}],["sc.                                                                                         ",{}]],"20":[["sc.__class__(                   sc.__weakref__                  sc.addFile(                          ",{}]],"21":[["sc.__del__(                     sc._accumulatorServer           sc.addPyFile(                        ",{}]],"22":[["sc.__delattr__(                 sc._active_spark_context        sc.batchSize                         ",{}]],"23":[["sc.__dict__                     sc._checkpointFile(             sc.broadcast(                        ",{}]],"24":[["sc.__doc__                      sc._ensure_initialized(         sc.clearFiles(                       ",{}]],"25":[["sc.__format__(                  sc._gateway(                    sc.defaultParallelism                ",{}]],"26":[["sc.__getattribute__(            sc._getJavaStorageLevel(        sc.environment                       ",{}]],"27":[["sc.__hash__(                    sc._javaAccumulator(            sc.jobName                           ",{}]],"28":[["sc.__init__(                    sc._jsc(                        sc.master                            ",{}]],"29":[["sc.__module__                   sc._jvm(                        sc.parallelize(                      ",{}]],"30":[["sc.__new__(                     sc._lock                        sc.pythonExec                        ",{}]],"31":[["sc.__reduce__(                  sc._next_accum_id               sc.setCheckpointDir(                 ",{}]],"32":[["sc.__reduce_ex__(               sc._pickled_broadcast_vars      sc.setSystemProperty(                ",{}]],"33":[["sc.__repr__(                    sc._python_includes             sc.sparkHome                         ",{}]],"34":[["sc.__setattr__(                 sc._takePartition(              sc.stop(                             ",{}]],"35":[["sc.__sizeof__(                  sc._temp_dir                    sc.textFile(                         ",{}]],"36":[["sc.__str__(                     sc._writeIteratorToPickleFile(  sc.union(                            ",{}]],"37":[["sc.__subclasshook__(            sc.accumulator(                                                      ",{}]],"38":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"39":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"40":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"41":[["                                                                                                     ",{}]]},"cursor":{"x":0}}],[0.03075,{"lines":{"0":[["14/04/13 17:04:59 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 192.168.1.",{}]],"1":[["102:63139 with 333.7 MB RAM                                                                          ",{}]],"2":[["14/04/13 17:04:59 INFO BlockManagerMaster: Registered BlockManager                                   ",{}]],"3":[["14/04/13 17:04:59 INFO HttpBroadcast: Broadcast server started at http://192.168.1.102:63140         ",{}]],"4":[["14/04/13 17:04:59 INFO SparkEnv: Registering MapOutputTracker                                        ",{}]],"5":[["14/04/13 17:04:59 INFO HttpFileServer: HTTP File server directory is /var/folders/_g/0qtqr8zs2jx7n86s",{}]],"6":[["gxw3r_nw0000gn/T/spark-80399ca2-19cc-4757-a712-bd7d80396cc5                                          ",{}]],"7":[["14/04/13 17:04:59 INFO SparkUI: Started Spark Web UI at http://192.168.1.102:4040                    ",{}]],"8":[["2014-04-13 17:05:00.160 java[7373:c103] Unable to load realm info from SCDynamicStore                ",{}]],"9":[["Welcome to                                                                                           ",{}]],"10":[["      ____              __                                                                           ",{}]],"11":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"12":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"13":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"14":[["      /_/                                                                                            ",{}]],"15":[["                                                                                                     ",{}]],"16":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"17":[["Spark context avaiable as sc.                                                                        ",{}]],"18":[["h[1] >>> ",{"fg":2}],["sc.                                                                                         ",{}]],"19":[["sc.__class__(                   sc.__weakref__                  sc.addFile(                          ",{}]],"20":[["sc.__del__(                     sc._accumulatorServer           sc.addPyFile(                        ",{}]],"21":[["sc.__delattr__(                 sc._active_spark_context        sc.batchSize                         ",{}]],"22":[["sc.__dict__                     sc._checkpointFile(             sc.broadcast(                        ",{}]],"23":[["sc.__doc__                      sc._ensure_initialized(         sc.clearFiles(                       ",{}]],"24":[["sc.__format__(                  sc._gateway(                    sc.defaultParallelism                ",{}]],"25":[["sc.__getattribute__(            sc._getJavaStorageLevel(        sc.environment                       ",{}]],"26":[["sc.__hash__(                    sc._javaAccumulator(            sc.jobName                           ",{}]],"27":[["sc.__init__(                    sc._jsc(                        sc.master                            ",{}]],"28":[["sc.__module__                   sc._jvm(                        sc.parallelize(                      ",{}]],"29":[["sc.__new__(                     sc._lock                        sc.pythonExec                        ",{}]],"30":[["sc.__reduce__(                  sc._next_accum_id               sc.setCheckpointDir(                 ",{}]],"31":[["sc.__reduce_ex__(               sc._pickled_broadcast_vars      sc.setSystemProperty(                ",{}]],"32":[["sc.__repr__(                    sc._python_includes             sc.sparkHome                         ",{}]],"33":[["sc.__setattr__(                 sc._takePartition(              sc.stop(                             ",{}]],"34":[["sc.__sizeof__(                  sc._temp_dir                    sc.textFile(                         ",{}]],"35":[["sc.__str__(                     sc._writeIteratorToPickleFile(  sc.union(                            ",{}]],"36":[["sc.__subclasshook__(            sc.accumulator(                                                      ",{}]],"37":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"38":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"39":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"40":[["14/04/13 17:05:29 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]]}}],[0.011291,{"lines":{"0":[["14/04/13 17:04:59 INFO HttpFileServer: HTTP File server directory is /var/folders/_g/0qtqr8zs2jx7n86s",{}]],"1":[["gxw3r_nw0000gn/T/spark-80399ca2-19cc-4757-a712-bd7d80396cc5                                          ",{}]],"2":[["14/04/13 17:04:59 INFO SparkUI: Started Spark Web UI at http://192.168.1.102:4040                    ",{}]],"3":[["2014-04-13 17:05:00.160 java[7373:c103] Unable to load realm info from SCDynamicStore                ",{}]],"4":[["Welcome to                                                                                           ",{}]],"5":[["      ____              __                                                                           ",{}]],"6":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"7":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"8":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"9":[["      /_/                                                                                            ",{}]],"10":[["                                                                                                     ",{}]],"11":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"12":[["Spark context avaiable as sc.                                                                        ",{}]],"13":[["h[1] >>> ",{"fg":2}],["sc.                                                                                         ",{}]],"14":[["sc.__class__(                   sc.__weakref__                  sc.addFile(                          ",{}]],"15":[["sc.__del__(                     sc._accumulatorServer           sc.addPyFile(                        ",{}]],"16":[["sc.__delattr__(                 sc._active_spark_context        sc.batchSize                         ",{}]],"17":[["sc.__dict__                     sc._checkpointFile(             sc.broadcast(                        ",{}]],"18":[["sc.__doc__                      sc._ensure_initialized(         sc.clearFiles(                       ",{}]],"19":[["sc.__format__(                  sc._gateway(                    sc.defaultParallelism                ",{}]],"20":[["sc.__getattribute__(            sc._getJavaStorageLevel(        sc.environment                       ",{}]],"21":[["sc.__hash__(                    sc._javaAccumulator(            sc.jobName                           ",{}]],"22":[["sc.__init__(                    sc._jsc(                        sc.master                            ",{}]],"23":[["sc.__module__                   sc._jvm(                        sc.parallelize(                      ",{}]],"24":[["sc.__new__(                     sc._lock                        sc.pythonExec                        ",{}]],"25":[["sc.__reduce__(                  sc._next_accum_id               sc.setCheckpointDir(                 ",{}]],"26":[["sc.__reduce_ex__(               sc._pickled_broadcast_vars      sc.setSystemProperty(                ",{}]],"27":[["sc.__repr__(                    sc._python_includes             sc.sparkHome                         ",{}]],"28":[["sc.__setattr__(                 sc._takePartition(              sc.stop(                             ",{}]],"29":[["sc.__sizeof__(                  sc._temp_dir                    sc.textFile(                         ",{}]],"30":[["sc.__str__(                     sc._writeIteratorToPickleFile(  sc.union(                            ",{}]],"31":[["sc.__subclasshook__(            sc.accumulator(                                                      ",{}]],"32":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"33":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"34":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"35":[["14/04/13 17:05:29 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"36":[["14/04/13 17:05:29 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"37":[["utput partitions (allowLocal=false)                                                                  ",{}]],"38":[["14/04/13 17:05:29 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"39":[["2)                                                                                                   ",{}]],"40":[["14/04/13 17:05:29 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]]}}],[0.016397,{"lines":{"0":[["2014-04-13 17:05:00.160 java[7373:c103] Unable to load realm info from SCDynamicStore                ",{}]],"1":[["Welcome to                                                                                           ",{}]],"2":[["      ____              __                                                                           ",{}]],"3":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"4":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"5":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"6":[["      /_/                                                                                            ",{}]],"7":[["                                                                                                     ",{}]],"8":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"9":[["Spark context avaiable as sc.                                                                        ",{}]],"10":[["h[1] >>> ",{"fg":2}],["sc.                                                                                         ",{}]],"11":[["sc.__class__(                   sc.__weakref__                  sc.addFile(                          ",{}]],"12":[["sc.__del__(                     sc._accumulatorServer           sc.addPyFile(                        ",{}]],"13":[["sc.__delattr__(                 sc._active_spark_context        sc.batchSize                         ",{}]],"14":[["sc.__dict__                     sc._checkpointFile(             sc.broadcast(                        ",{}]],"15":[["sc.__doc__                      sc._ensure_initialized(         sc.clearFiles(                       ",{}]],"16":[["sc.__format__(                  sc._gateway(                    sc.defaultParallelism                ",{}]],"17":[["sc.__getattribute__(            sc._getJavaStorageLevel(        sc.environment                       ",{}]],"18":[["sc.__hash__(                    sc._javaAccumulator(            sc.jobName                           ",{}]],"19":[["sc.__init__(                    sc._jsc(                        sc.master                            ",{}]],"20":[["sc.__module__                   sc._jvm(                        sc.parallelize(                      ",{}]],"21":[["sc.__new__(                     sc._lock                        sc.pythonExec                        ",{}]],"22":[["sc.__reduce__(                  sc._next_accum_id               sc.setCheckpointDir(                 ",{}]],"23":[["sc.__reduce_ex__(               sc._pickled_broadcast_vars      sc.setSystemProperty(                ",{}]],"24":[["sc.__repr__(                    sc._python_includes             sc.sparkHome                         ",{}]],"25":[["sc.__setattr__(                 sc._takePartition(              sc.stop(                             ",{}]],"26":[["sc.__sizeof__(                  sc._temp_dir                    sc.textFile(                         ",{}]],"27":[["sc.__str__(                     sc._writeIteratorToPickleFile(  sc.union(                            ",{}]],"28":[["sc.__subclasshook__(            sc.accumulator(                                                      ",{}]],"29":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"30":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"31":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"32":[["14/04/13 17:05:29 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"33":[["14/04/13 17:05:29 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"34":[["utput partitions (allowLocal=false)                                                                  ",{}]],"35":[["14/04/13 17:05:29 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"36":[["2)                                                                                                   ",{}]],"37":[["14/04/13 17:05:29 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"38":[["14/04/13 17:05:29 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"39":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"40":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]]}}],[0.06295000000000001,{"lines":{"0":[["      ____              __                                                                           ",{}]],"1":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"2":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"3":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"4":[["      /_/                                                                                            ",{}]],"5":[["                                                                                                     ",{}]],"6":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"7":[["Spark context avaiable as sc.                                                                        ",{}]],"8":[["h[1] >>> ",{"fg":2}],["sc.                                                                                         ",{}]],"9":[["sc.__class__(                   sc.__weakref__                  sc.addFile(                          ",{}]],"10":[["sc.__del__(                     sc._accumulatorServer           sc.addPyFile(                        ",{}]],"11":[["sc.__delattr__(                 sc._active_spark_context        sc.batchSize                         ",{}]],"12":[["sc.__dict__                     sc._checkpointFile(             sc.broadcast(                        ",{}]],"13":[["sc.__doc__                      sc._ensure_initialized(         sc.clearFiles(                       ",{}]],"14":[["sc.__format__(                  sc._gateway(                    sc.defaultParallelism                ",{}]],"15":[["sc.__getattribute__(            sc._getJavaStorageLevel(        sc.environment                       ",{}]],"16":[["sc.__hash__(                    sc._javaAccumulator(            sc.jobName                           ",{}]],"17":[["sc.__init__(                    sc._jsc(                        sc.master                            ",{}]],"18":[["sc.__module__                   sc._jvm(                        sc.parallelize(                      ",{}]],"19":[["sc.__new__(                     sc._lock                        sc.pythonExec                        ",{}]],"20":[["sc.__reduce__(                  sc._next_accum_id               sc.setCheckpointDir(                 ",{}]],"21":[["sc.__reduce_ex__(               sc._pickled_broadcast_vars      sc.setSystemProperty(                ",{}]],"22":[["sc.__repr__(                    sc._python_includes             sc.sparkHome                         ",{}]],"23":[["sc.__setattr__(                 sc._takePartition(              sc.stop(                             ",{}]],"24":[["sc.__sizeof__(                  sc._temp_dir                    sc.textFile(                         ",{}]],"25":[["sc.__str__(                     sc._writeIteratorToPickleFile(  sc.union(                            ",{}]],"26":[["sc.__subclasshook__(            sc.accumulator(                                                      ",{}]],"27":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"28":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"29":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"30":[["14/04/13 17:05:29 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"31":[["14/04/13 17:05:29 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"32":[["utput partitions (allowLocal=false)                                                                  ",{}]],"33":[["14/04/13 17:05:29 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"34":[["2)                                                                                                   ",{}]],"35":[["14/04/13 17:05:29 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"36":[["14/04/13 17:05:29 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"37":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"38":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"39":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"40":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]]}}],[0.021871,{"lines":{"0":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"1":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"2":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"3":[["      /_/                                                                                            ",{}]],"4":[["                                                                                                     ",{}]],"5":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"6":[["Spark context avaiable as sc.                                                                        ",{}]],"7":[["h[1] >>> ",{"fg":2}],["sc.                                                                                         ",{}]],"8":[["sc.__class__(                   sc.__weakref__                  sc.addFile(                          ",{}]],"9":[["sc.__del__(                     sc._accumulatorServer           sc.addPyFile(                        ",{}]],"10":[["sc.__delattr__(                 sc._active_spark_context        sc.batchSize                         ",{}]],"11":[["sc.__dict__                     sc._checkpointFile(             sc.broadcast(                        ",{}]],"12":[["sc.__doc__                      sc._ensure_initialized(         sc.clearFiles(                       ",{}]],"13":[["sc.__format__(                  sc._gateway(                    sc.defaultParallelism                ",{}]],"14":[["sc.__getattribute__(            sc._getJavaStorageLevel(        sc.environment                       ",{}]],"15":[["sc.__hash__(                    sc._javaAccumulator(            sc.jobName                           ",{}]],"16":[["sc.__init__(                    sc._jsc(                        sc.master                            ",{}]],"17":[["sc.__module__                   sc._jvm(                        sc.parallelize(                      ",{}]],"18":[["sc.__new__(                     sc._lock                        sc.pythonExec                        ",{}]],"19":[["sc.__reduce__(                  sc._next_accum_id               sc.setCheckpointDir(                 ",{}]],"20":[["sc.__reduce_ex__(               sc._pickled_broadcast_vars      sc.setSystemProperty(                ",{}]],"21":[["sc.__repr__(                    sc._python_includes             sc.sparkHome                         ",{}]],"22":[["sc.__setattr__(                 sc._takePartition(              sc.stop(                             ",{}]],"23":[["sc.__sizeof__(                  sc._temp_dir                    sc.textFile(                         ",{}]],"24":[["sc.__str__(                     sc._writeIteratorToPickleFile(  sc.union(                            ",{}]],"25":[["sc.__subclasshook__(            sc.accumulator(                                                      ",{}]],"26":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"27":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"28":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"29":[["14/04/13 17:05:29 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"30":[["14/04/13 17:05:29 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"31":[["utput partitions (allowLocal=false)                                                                  ",{}]],"32":[["14/04/13 17:05:29 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"33":[["2)                                                                                                   ",{}]],"34":[["14/04/13 17:05:29 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"35":[["14/04/13 17:05:29 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"36":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"37":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"38":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"39":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]],"40":[["14/04/13 17:05:29 INFO LocalTaskSetManager: Size of task 0 is 1215 bytes                             ",{}]]}}],[0.010955,{"lines":{"0":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"1":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"2":[["      /_/                                                                                            ",{}]],"3":[["                                                                                                     ",{}]],"4":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"5":[["Spark context avaiable as sc.                                                                        ",{}]],"6":[["h[1] >>> ",{"fg":2}],["sc.                                                                                         ",{}]],"7":[["sc.__class__(                   sc.__weakref__                  sc.addFile(                          ",{}]],"8":[["sc.__del__(                     sc._accumulatorServer           sc.addPyFile(                        ",{}]],"9":[["sc.__delattr__(                 sc._active_spark_context        sc.batchSize                         ",{}]],"10":[["sc.__dict__                     sc._checkpointFile(             sc.broadcast(                        ",{}]],"11":[["sc.__doc__                      sc._ensure_initialized(         sc.clearFiles(                       ",{}]],"12":[["sc.__format__(                  sc._gateway(                    sc.defaultParallelism                ",{}]],"13":[["sc.__getattribute__(            sc._getJavaStorageLevel(        sc.environment                       ",{}]],"14":[["sc.__hash__(                    sc._javaAccumulator(            sc.jobName                           ",{}]],"15":[["sc.__init__(                    sc._jsc(                        sc.master                            ",{}]],"16":[["sc.__module__                   sc._jvm(                        sc.parallelize(                      ",{}]],"17":[["sc.__new__(                     sc._lock                        sc.pythonExec                        ",{}]],"18":[["sc.__reduce__(                  sc._next_accum_id               sc.setCheckpointDir(                 ",{}]],"19":[["sc.__reduce_ex__(               sc._pickled_broadcast_vars      sc.setSystemProperty(                ",{}]],"20":[["sc.__repr__(                    sc._python_includes             sc.sparkHome                         ",{}]],"21":[["sc.__setattr__(                 sc._takePartition(              sc.stop(                             ",{}]],"22":[["sc.__sizeof__(                  sc._temp_dir                    sc.textFile(                         ",{}]],"23":[["sc.__str__(                     sc._writeIteratorToPickleFile(  sc.union(                            ",{}]],"24":[["sc.__subclasshook__(            sc.accumulator(                                                      ",{}]],"25":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"26":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"27":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"28":[["14/04/13 17:05:29 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"29":[["14/04/13 17:05:29 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"30":[["utput partitions (allowLocal=false)                                                                  ",{}]],"31":[["14/04/13 17:05:29 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"32":[["2)                                                                                                   ",{}]],"33":[["14/04/13 17:05:29 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"34":[["14/04/13 17:05:29 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"35":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"36":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"37":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"38":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]],"39":[["14/04/13 17:05:29 INFO LocalTaskSetManager: Size of task 0 is 1215 bytes                             ",{}]],"40":[["14/04/13 17:05:29 INFO Executor: Running task ID 0                                                   ",{}]]}}],[0.039267,{"lines":{"0":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"1":[["      /_/                                                                                            ",{}]],"2":[["                                                                                                     ",{}]],"3":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"4":[["Spark context avaiable as sc.                                                                        ",{}]],"5":[["h[1] >>> ",{"fg":2}],["sc.                                                                                         ",{}]],"6":[["sc.__class__(                   sc.__weakref__                  sc.addFile(                          ",{}]],"7":[["sc.__del__(                     sc._accumulatorServer           sc.addPyFile(                        ",{}]],"8":[["sc.__delattr__(                 sc._active_spark_context        sc.batchSize                         ",{}]],"9":[["sc.__dict__                     sc._checkpointFile(             sc.broadcast(                        ",{}]],"10":[["sc.__doc__                      sc._ensure_initialized(         sc.clearFiles(                       ",{}]],"11":[["sc.__format__(                  sc._gateway(                    sc.defaultParallelism                ",{}]],"12":[["sc.__getattribute__(            sc._getJavaStorageLevel(        sc.environment                       ",{}]],"13":[["sc.__hash__(                    sc._javaAccumulator(            sc.jobName                           ",{}]],"14":[["sc.__init__(                    sc._jsc(                        sc.master                            ",{}]],"15":[["sc.__module__                   sc._jvm(                        sc.parallelize(                      ",{}]],"16":[["sc.__new__(                     sc._lock                        sc.pythonExec                        ",{}]],"17":[["sc.__reduce__(                  sc._next_accum_id               sc.setCheckpointDir(                 ",{}]],"18":[["sc.__reduce_ex__(               sc._pickled_broadcast_vars      sc.setSystemProperty(                ",{}]],"19":[["sc.__repr__(                    sc._python_includes             sc.sparkHome                         ",{}]],"20":[["sc.__setattr__(                 sc._takePartition(              sc.stop(                             ",{}]],"21":[["sc.__sizeof__(                  sc._temp_dir                    sc.textFile(                         ",{}]],"22":[["sc.__str__(                     sc._writeIteratorToPickleFile(  sc.union(                            ",{}]],"23":[["sc.__subclasshook__(            sc.accumulator(                                                      ",{}]],"24":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"25":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"26":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"27":[["14/04/13 17:05:29 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"28":[["14/04/13 17:05:29 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"29":[["utput partitions (allowLocal=false)                                                                  ",{}]],"30":[["14/04/13 17:05:29 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"31":[["2)                                                                                                   ",{}]],"32":[["14/04/13 17:05:29 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"33":[["14/04/13 17:05:29 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"34":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"35":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"36":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"37":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]],"38":[["14/04/13 17:05:29 INFO LocalTaskSetManager: Size of task 0 is 1215 bytes                             ",{}]],"39":[["14/04/13 17:05:29 INFO Executor: Running task ID 0                                                   ",{}]],"40":[["14/04/13 17:05:29 INFO Executor: Serialized size of result for 0 is 517                              ",{}]]}}],[0.015346,{"lines":{"0":[["sc.__delattr__(                 sc._active_spark_context        sc.batchSize                         ",{}]],"1":[["sc.__dict__                     sc._checkpointFile(             sc.broadcast(                        ",{}]],"2":[["sc.__doc__                      sc._ensure_initialized(         sc.clearFiles(                       ",{}]],"3":[["sc.__format__(                  sc._gateway(                    sc.defaultParallelism                ",{}]],"4":[["sc.__getattribute__(            sc._getJavaStorageLevel(        sc.environment                       ",{}]],"5":[["sc.__hash__(                    sc._javaAccumulator(            sc.jobName                           ",{}]],"6":[["sc.__init__(                    sc._jsc(                        sc.master                            ",{}]],"7":[["sc.__module__                   sc._jvm(                        sc.parallelize(                      ",{}]],"8":[["sc.__new__(                     sc._lock                        sc.pythonExec                        ",{}]],"9":[["sc.__reduce__(                  sc._next_accum_id               sc.setCheckpointDir(                 ",{}]],"10":[["sc.__reduce_ex__(               sc._pickled_broadcast_vars      sc.setSystemProperty(                ",{}]],"11":[["sc.__repr__(                    sc._python_includes             sc.sparkHome                         ",{}]],"12":[["sc.__setattr__(                 sc._takePartition(              sc.stop(                             ",{}]],"13":[["sc.__sizeof__(                  sc._temp_dir                    sc.textFile(                         ",{}]],"14":[["sc.__str__(                     sc._writeIteratorToPickleFile(  sc.union(                            ",{}]],"15":[["sc.__subclasshook__(            sc.accumulator(                                                      ",{}]],"16":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"17":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"18":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"19":[["14/04/13 17:05:29 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"20":[["14/04/13 17:05:29 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"21":[["utput partitions (allowLocal=false)                                                                  ",{}]],"22":[["14/04/13 17:05:29 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"23":[["2)                                                                                                   ",{}]],"24":[["14/04/13 17:05:29 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"25":[["14/04/13 17:05:29 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"26":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"27":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"28":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"29":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]],"30":[["14/04/13 17:05:29 INFO LocalTaskSetManager: Size of task 0 is 1215 bytes                             ",{}]],"31":[["14/04/13 17:05:29 INFO Executor: Running task ID 0                                                   ",{}]],"32":[["14/04/13 17:05:29 INFO Executor: Serialized size of result for 0 is 517                              ",{}]],"33":[["14/04/13 17:05:29 INFO Executor: Sending result for 0 directly to driver                             ",{}]],"34":[["14/04/13 17:05:29 INFO DAGScheduler: Completed ResultTask(0, 0)                                      ",{}]],"35":[["14/04/13 17:05:29 INFO DAGScheduler: Stage 0 (collect at NativeMethodAccessorImpl.java:-2) finished i",{}]],"36":[["n 0.070 s                                                                                            ",{}]],"37":[["14/04/13 17:05:29 INFO LocalScheduler: Remove TaskSet 0.0 from pool                                  ",{}]],"38":[["14/04/13 17:05:29 INFO SparkContext: Job finished: collect at NativeMethodAccessorImpl.java:-2, took ",{}]],"39":[["0.175037 s                                                                                           ",{}]],"40":[["14/04/13 17:05:29 INFO Executor: Finished task ID 0                                                  ",{}]]}}],[0.008786,{"lines":{"0":[["sc.__dict__                     sc._checkpointFile(             sc.broadcast(                        ",{}]],"1":[["sc.__doc__                      sc._ensure_initialized(         sc.clearFiles(                       ",{}]],"2":[["sc.__format__(                  sc._gateway(                    sc.defaultParallelism                ",{}]],"3":[["sc.__getattribute__(            sc._getJavaStorageLevel(        sc.environment                       ",{}]],"4":[["sc.__hash__(                    sc._javaAccumulator(            sc.jobName                           ",{}]],"5":[["sc.__init__(                    sc._jsc(                        sc.master                            ",{}]],"6":[["sc.__module__                   sc._jvm(                        sc.parallelize(                      ",{}]],"7":[["sc.__new__(                     sc._lock                        sc.pythonExec                        ",{}]],"8":[["sc.__reduce__(                  sc._next_accum_id               sc.setCheckpointDir(                 ",{}]],"9":[["sc.__reduce_ex__(               sc._pickled_broadcast_vars      sc.setSystemProperty(                ",{}]],"10":[["sc.__repr__(                    sc._python_includes             sc.sparkHome                         ",{}]],"11":[["sc.__setattr__(                 sc._takePartition(              sc.stop(                             ",{}]],"12":[["sc.__sizeof__(                  sc._temp_dir                    sc.textFile(                         ",{}]],"13":[["sc.__str__(                     sc._writeIteratorToPickleFile(  sc.union(                            ",{}]],"14":[["sc.__subclasshook__(            sc.accumulator(                                                      ",{}]],"15":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"16":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"17":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"18":[["14/04/13 17:05:29 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"19":[["14/04/13 17:05:29 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"20":[["utput partitions (allowLocal=false)                                                                  ",{}]],"21":[["14/04/13 17:05:29 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"22":[["2)                                                                                                   ",{}]],"23":[["14/04/13 17:05:29 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"24":[["14/04/13 17:05:29 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"25":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"26":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"27":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"28":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]],"29":[["14/04/13 17:05:29 INFO LocalTaskSetManager: Size of task 0 is 1215 bytes                             ",{}]],"30":[["14/04/13 17:05:29 INFO Executor: Running task ID 0                                                   ",{}]],"31":[["14/04/13 17:05:29 INFO Executor: Serialized size of result for 0 is 517                              ",{}]],"32":[["14/04/13 17:05:29 INFO Executor: Sending result for 0 directly to driver                             ",{}]],"33":[["14/04/13 17:05:29 INFO DAGScheduler: Completed ResultTask(0, 0)                                      ",{}]],"34":[["14/04/13 17:05:29 INFO DAGScheduler: Stage 0 (collect at NativeMethodAccessorImpl.java:-2) finished i",{}]],"35":[["n 0.070 s                                                                                            ",{}]],"36":[["14/04/13 17:05:29 INFO LocalScheduler: Remove TaskSet 0.0 from pool                                  ",{}]],"37":[["14/04/13 17:05:29 INFO SparkContext: Job finished: collect at NativeMethodAccessorImpl.java:-2, took ",{}]],"38":[["0.175037 s                                                                                           ",{}]],"39":[["14/04/13 17:05:29 INFO Executor: Finished task ID 0                                                  ",{}]],"40":[["[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]                                                                       ",{}]],"41":[["h[2] >>> ",{"fg":2}],["                                                                                            ",{}]]},"cursor":{"x":9}}],[2.196107,{"lines":{"0":[["sc.__doc__                      sc._ensure_initialized(         sc.clearFiles(                       ",{}]],"1":[["sc.__format__(                  sc._gateway(                    sc.defaultParallelism                ",{}]],"2":[["sc.__getattribute__(            sc._getJavaStorageLevel(        sc.environment                       ",{}]],"3":[["sc.__hash__(                    sc._javaAccumulator(            sc.jobName                           ",{}]],"4":[["sc.__init__(                    sc._jsc(                        sc.master                            ",{}]],"5":[["sc.__module__                   sc._jvm(                        sc.parallelize(                      ",{}]],"6":[["sc.__new__(                     sc._lock                        sc.pythonExec                        ",{}]],"7":[["sc.__reduce__(                  sc._next_accum_id               sc.setCheckpointDir(                 ",{}]],"8":[["sc.__reduce_ex__(               sc._pickled_broadcast_vars      sc.setSystemProperty(                ",{}]],"9":[["sc.__repr__(                    sc._python_includes             sc.sparkHome                         ",{}]],"10":[["sc.__setattr__(                 sc._takePartition(              sc.stop(                             ",{}]],"11":[["sc.__sizeof__(                  sc._temp_dir                    sc.textFile(                         ",{}]],"12":[["sc.__str__(                     sc._writeIteratorToPickleFile(  sc.union(                            ",{}]],"13":[["sc.__subclasshook__(            sc.accumulator(                                                      ",{}]],"14":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"15":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"16":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"17":[["14/04/13 17:05:29 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"18":[["14/04/13 17:05:29 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"19":[["utput partitions (allowLocal=false)                                                                  ",{}]],"20":[["14/04/13 17:05:29 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"21":[["2)                                                                                                   ",{}]],"22":[["14/04/13 17:05:29 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"23":[["14/04/13 17:05:29 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"24":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"25":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"26":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"27":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]],"28":[["14/04/13 17:05:29 INFO LocalTaskSetManager: Size of task 0 is 1215 bytes                             ",{}]],"29":[["14/04/13 17:05:29 INFO Executor: Running task ID 0                                                   ",{}]],"30":[["14/04/13 17:05:29 INFO Executor: Serialized size of result for 0 is 517                              ",{}]],"31":[["14/04/13 17:05:29 INFO Executor: Sending result for 0 directly to driver                             ",{}]],"32":[["14/04/13 17:05:29 INFO DAGScheduler: Completed ResultTask(0, 0)                                      ",{}]],"33":[["14/04/13 17:05:29 INFO DAGScheduler: Stage 0 (collect at NativeMethodAccessorImpl.java:-2) finished i",{}]],"34":[["n 0.070 s                                                                                            ",{}]],"35":[["14/04/13 17:05:29 INFO LocalScheduler: Remove TaskSet 0.0 from pool                                  ",{}]],"36":[["14/04/13 17:05:29 INFO SparkContext: Job finished: collect at NativeMethodAccessorImpl.java:-2, took ",{}]],"37":[["0.175037 s                                                                                           ",{}]],"38":[["14/04/13 17:05:29 INFO Executor: Finished task ID 0                                                  ",{}]],"39":[["[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]                                                                       ",{}]],"40":[["h[2] >>> ",{"fg":2}],["                                                                                            ",{}]],"41":[["                                                                                                     ",{}]]},"cursor":{"x":0}}],[0.011889,{}],[0.035694,{"lines":{"0":[["sc.__format__(                  sc._gateway(                    sc.defaultParallelism                ",{}]],"1":[["sc.__getattribute__(            sc._getJavaStorageLevel(        sc.environment                       ",{}]],"2":[["sc.__hash__(                    sc._javaAccumulator(            sc.jobName                           ",{}]],"3":[["sc.__init__(                    sc._jsc(                        sc.master                            ",{}]],"4":[["sc.__module__                   sc._jvm(                        sc.parallelize(                      ",{}]],"5":[["sc.__new__(                     sc._lock                        sc.pythonExec                        ",{}]],"6":[["sc.__reduce__(                  sc._next_accum_id               sc.setCheckpointDir(                 ",{}]],"7":[["sc.__reduce_ex__(               sc._pickled_broadcast_vars      sc.setSystemProperty(                ",{}]],"8":[["sc.__repr__(                    sc._python_includes             sc.sparkHome                         ",{}]],"9":[["sc.__setattr__(                 sc._takePartition(              sc.stop(                             ",{}]],"10":[["sc.__sizeof__(                  sc._temp_dir                    sc.textFile(                         ",{}]],"11":[["sc.__str__(                     sc._writeIteratorToPickleFile(  sc.union(                            ",{}]],"12":[["sc.__subclasshook__(            sc.accumulator(                                                      ",{}]],"13":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"14":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"15":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"16":[["14/04/13 17:05:29 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"17":[["14/04/13 17:05:29 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"18":[["utput partitions (allowLocal=false)                                                                  ",{}]],"19":[["14/04/13 17:05:29 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"20":[["2)                                                                                                   ",{}]],"21":[["14/04/13 17:05:29 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"22":[["14/04/13 17:05:29 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"23":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"24":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"25":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"26":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]],"27":[["14/04/13 17:05:29 INFO LocalTaskSetManager: Size of task 0 is 1215 bytes                             ",{}]],"28":[["14/04/13 17:05:29 INFO Executor: Running task ID 0                                                   ",{}]],"29":[["14/04/13 17:05:29 INFO Executor: Serialized size of result for 0 is 517                              ",{}]],"30":[["14/04/13 17:05:29 INFO Executor: Sending result for 0 directly to driver                             ",{}]],"31":[["14/04/13 17:05:29 INFO DAGScheduler: Completed ResultTask(0, 0)                                      ",{}]],"32":[["14/04/13 17:05:29 INFO DAGScheduler: Stage 0 (collect at NativeMethodAccessorImpl.java:-2) finished i",{}]],"33":[["n 0.070 s                                                                                            ",{}]],"34":[["14/04/13 17:05:29 INFO LocalScheduler: Remove TaskSet 0.0 from pool                                  ",{}]],"35":[["14/04/13 17:05:29 INFO SparkContext: Job finished: collect at NativeMethodAccessorImpl.java:-2, took ",{}]],"36":[["0.175037 s                                                                                           ",{}]],"37":[["14/04/13 17:05:29 INFO Executor: Finished task ID 0                                                  ",{}]],"38":[["[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]                                                                       ",{}]],"39":[["h[2] >>> ",{"fg":2}],["                                                                                            ",{}]],"40":[["╭─",{}],["zzl",{"fg":40}],[" ",{}],["at",{"fg":239}],[" ",{}],["zhus-MacBook-Air-3",{"fg":33}],[" ",{}],["in",{"fg":239}],[" ",{}],["~/projects/scala",{"fg":226,"bold":true}],[" ",{}],["using",{"fg":239}],[" ‹›",{"fg":243}],["                                             ",{}]],"41":[["╰─○                                                                                                  ",{}]]},"cursor":{"x":4}}],[0.63867,{"lines":{"41":[["╰─○ e                                                                                                ",{}]]},"cursor":{"x":5}}],[0.308697,{"lines":{"41":[["╰─○ ex                                                                                               ",{}]]},"cursor":{"x":6}}],[0.127289,{"lines":{"41":[["╰─○ exi                                                                                              ",{}]]},"cursor":{"x":7}}],[0.144656,{"lines":{"41":[["╰─○ exit                                                                                             ",{}]]},"cursor":{"x":8}}],[0.210277,{"lines":{"0":[["sc.__getattribute__(            sc._getJavaStorageLevel(        sc.environment                       ",{}]],"1":[["sc.__hash__(                    sc._javaAccumulator(            sc.jobName                           ",{}]],"2":[["sc.__init__(                    sc._jsc(                        sc.master                            ",{}]],"3":[["sc.__module__                   sc._jvm(                        sc.parallelize(                      ",{}]],"4":[["sc.__new__(                     sc._lock                        sc.pythonExec                        ",{}]],"5":[["sc.__reduce__(                  sc._next_accum_id               sc.setCheckpointDir(                 ",{}]],"6":[["sc.__reduce_ex__(               sc._pickled_broadcast_vars      sc.setSystemProperty(                ",{}]],"7":[["sc.__repr__(                    sc._python_includes             sc.sparkHome                         ",{}]],"8":[["sc.__setattr__(                 sc._takePartition(              sc.stop(                             ",{}]],"9":[["sc.__sizeof__(                  sc._temp_dir                    sc.textFile(                         ",{}]],"10":[["sc.__str__(                     sc._writeIteratorToPickleFile(  sc.union(                            ",{}]],"11":[["sc.__subclasshook__(            sc.accumulator(                                                      ",{}]],"12":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"13":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"14":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"15":[["14/04/13 17:05:29 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"16":[["14/04/13 17:05:29 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"17":[["utput partitions (allowLocal=false)                                                                  ",{}]],"18":[["14/04/13 17:05:29 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"19":[["2)                                                                                                   ",{}]],"20":[["14/04/13 17:05:29 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"21":[["14/04/13 17:05:29 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"22":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"23":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"24":[["14/04/13 17:05:29 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"25":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]],"26":[["14/04/13 17:05:29 INFO LocalTaskSetManager: Size of task 0 is 1215 bytes                             ",{}]],"27":[["14/04/13 17:05:29 INFO Executor: Running task ID 0                                                   ",{}]],"28":[["14/04/13 17:05:29 INFO Executor: Serialized size of result for 0 is 517                              ",{}]],"29":[["14/04/13 17:05:29 INFO Executor: Sending result for 0 directly to driver                             ",{}]],"30":[["14/04/13 17:05:29 INFO DAGScheduler: Completed ResultTask(0, 0)                                      ",{}]],"31":[["14/04/13 17:05:29 INFO DAGScheduler: Stage 0 (collect at NativeMethodAccessorImpl.java:-2) finished i",{}]],"32":[["n 0.070 s                                                                                            ",{}]],"33":[["14/04/13 17:05:29 INFO LocalScheduler: Remove TaskSet 0.0 from pool                                  ",{}]],"34":[["14/04/13 17:05:29 INFO SparkContext: Job finished: collect at NativeMethodAccessorImpl.java:-2, took ",{}]],"35":[["0.175037 s                                                                                           ",{}]],"36":[["14/04/13 17:05:29 INFO Executor: Finished task ID 0                                                  ",{}]],"37":[["[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]                                                                       ",{}]],"38":[["h[2] >>> ",{"fg":2}],["                                                                                            ",{}]],"39":[["╭─",{}],["zzl",{"fg":40}],[" ",{}],["at",{"fg":239}],[" ",{}],["zhus-MacBook-Air-3",{"fg":33}],[" ",{}],["in",{"fg":239}],[" ",{}],["~/projects/scala",{"fg":226,"bold":true}],[" ",{}],["using",{"fg":239}],[" ‹›",{"fg":243}],["                                             ",{}]],"40":[["╰─○ exit                                                                                             ",{}]],"41":[["                                                                                                     ",{}]]},"cursor":{"x":0}}],[0.001105,{}]]
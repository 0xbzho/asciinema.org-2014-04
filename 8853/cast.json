[[0.169817,{"lines":{"0":[["                                                                                                     ",{}]],"1":[["                                                                                                     ",{}]],"2":[["                                                                                                     ",{}]],"3":[["                                                                                                     ",{}]],"4":[["                                                                                                     ",{}]],"5":[["                                                                                                     ",{}]],"6":[["                                                                                                     ",{}]],"7":[["                                                                                                     ",{}]],"8":[["                                                                                                     ",{}]],"9":[["                                                                                                     ",{}]],"10":[["                                                                                                     ",{}]],"11":[["                                                                                                     ",{}]],"12":[["                                                                                                     ",{}]],"13":[["                                                                                                     ",{}]],"14":[["                                                                                                     ",{}]],"15":[["                                                                                                     ",{}]],"16":[["                                                                                                     ",{}]],"17":[["                                                                                                     ",{}]],"18":[["                                                                                                     ",{}]],"19":[["                                                                                                     ",{}]],"20":[["                                                                                                     ",{}]],"21":[["                                                                                                     ",{}]],"22":[["                                                                                                     ",{}]],"23":[["                                                                                                     ",{}]],"24":[["                                                                                                     ",{}]],"25":[["                                                                                                     ",{}]],"26":[["                                                                                                     ",{}]],"27":[["                                                                                                     ",{}]],"28":[["                                                                                                     ",{}]],"29":[["                                                                                                     ",{}]],"30":[["                                                                                                     ",{}]],"31":[["                                                                                                     ",{}]],"32":[["                                                                                                     ",{}]],"33":[["                                                                                                     ",{}]],"34":[["                                                                                                     ",{}]],"35":[["                                                                                                     ",{}]],"36":[["                                                                                                     ",{}]],"37":[["                                                                                                     ",{}]],"38":[["                                                                                                     ",{}]],"39":[["                                                                                                     ",{}]],"40":[["                                                                                                     ",{}]],"41":[["                                                                                                     ",{}]]},"cursor":{"x":0,"y":0,"visible":true}}],[0.047791,{"lines":{"0":[["╭─",{}],["zzl",{"fg":40}],[" ",{}],["at",{"fg":239}],[" ",{}],["zhus-MacBook-Air-3",{"fg":33}],[" ",{}],["in",{"fg":239}],[" ",{}],["~/projects/scala",{"fg":226,"bold":true}],[" ",{}],["using",{"fg":239}],[" ‹›",{"fg":243}],["                                             ",{}]],"1":[["╰─○                                                                                                  ",{}]]},"cursor":{"x":4,"y":1}}],[0.764303,{"lines":{"1":[["╰─○ p                                                                                                ",{}]]},"cursor":{"x":5}}],[0.098068,{"lines":{"1":[["╰─○ py                                                                                               ",{}]]},"cursor":{"x":6}}],[0.842525,{"lines":{"1":[["╰─○ pys                                                                                              ",{}]]},"cursor":{"x":7}}],[0.278636,{"lines":{"1":[["╰─○ pysp                                                                                             ",{}]]},"cursor":{"x":8}}],[0.170957,{"lines":{"1":[["╰─○ pyspark                                                                                          ",{}]]},"cursor":{"x":11}}],[0.853139,{"cursor":{"x":0,"y":2}}],[0.015019,{"lines":{"2":[["/Users/zzl/anaconda/bin/python                                                                       ",{}]]},"cursor":{"y":3}}],[0.025535,{"lines":{"3":[["Python 2.7.3 |Anaconda 1.4.0 (x86_64)| (default, Feb 25 2013, 18:45:56)                              ",{}]],"4":[["[GCC 4.0.1 (Apple Inc. build 5493)] on darwin                                                        ",{}]],"5":[["Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.                               ",{}]]},"cursor":{"y":6}}],[0.100535,{}],[1.181283,{"lines":{"6":[["14/04/13 17:02:56 INFO Slf4jEventHandler: Slf4jEventHandler started                                  ",{}]]},"cursor":{"y":7}}],[0.172522,{"lines":{"7":[["14/04/13 17:02:56 INFO SparkEnv: Registering BlockManagerMaster                                      ",{}]]},"cursor":{"y":8}}],[0.045037,{"lines":{"8":[["14/04/13 17:02:56 INFO DiskBlockManager: Created local directory at /var/folders/_g/0qtqr8zs2jx7n86sg",{}]],"9":[["xw3r_nw0000gn/T/spark-local-20140413170256-bc56                                                      ",{}]]},"cursor":{"y":10}}],[0.007891,{"lines":{"10":[["14/04/13 17:02:56 INFO MemoryStore: MemoryStore started with capacity 333.7 MB.                      ",{}]]},"cursor":{"y":11}}],[0.026053,{"lines":{"11":[["14/04/13 17:02:56 INFO ConnectionManager: Bound socket to port 63069 with id = ConnectionManagerId(19",{}]],"12":[["2.168.1.102,63069)                                                                                   ",{}]]},"cursor":{"y":13}}],[0.009167000000000002,{"lines":{"13":[["14/04/13 17:02:56 INFO BlockManagerMaster: Trying to register BlockManager                           ",{}]],"14":[["14/04/13 17:02:56 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 192.168.1.",{}]],"15":[["102:63069 with 333.7 MB RAM                                                                          ",{}]],"16":[["14/04/13 17:02:56 INFO BlockManagerMaster: Registered BlockManager                                   ",{}]]},"cursor":{"y":17}}],[0.13273,{"lines":{"17":[["14/04/13 17:02:56 INFO HttpBroadcast: Broadcast server started at http://192.168.1.102:63070         ",{}]]},"cursor":{"y":18}}],[0.009395000000000001,{"lines":{"18":[["14/04/13 17:02:56 INFO SparkEnv: Registering MapOutputTracker                                        ",{}]],"19":[["14/04/13 17:02:56 INFO HttpFileServer: HTTP File server directory is /var/folders/_g/0qtqr8zs2jx7n86s",{}]],"20":[["gxw3r_nw0000gn/T/spark-0f29d47d-3db6-4956-9af8-c12ef6a5f0b0                                          ",{}]]},"cursor":{"y":21}}],[0.206747,{"lines":{"21":[["14/04/13 17:02:57 INFO SparkUI: Started Spark Web UI at http://192.168.1.102:4040                    ",{}]]},"cursor":{"y":22}}],[0.203563,{"lines":{"22":[["2014-04-13 17:02:57.213 java[7240:c003] Unable to load realm info from SCDynamicStore                ",{}]]},"cursor":{"y":23}}],[0.144959,{"lines":{"23":[["Welcome to                                                                                           ",{}]],"24":[["      ____              __                                                                           ",{}]],"25":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"26":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"27":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"28":[["      /_/                                                                                            ",{}]]},"cursor":{"y":30}}],[0.003194,{"lines":{"30":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"31":[["Spark context avaiable as sc.                                                                        ",{}]],"32":[["h[1] >>> ",{"fg":2}],["                                                                                            ",{}]]},"cursor":{"x":9,"y":32}}],[2.805594,{"lines":{"32":[["h[1] >>> ",{"fg":2}],["n                                                                                           ",{}]]},"cursor":{"x":10}}],[0.207011,{"lines":{"32":[["h[1] >>> ",{"fg":2}],["nu                                                                                          ",{}]]},"cursor":{"x":11}}],[0.213952,{"lines":{"32":[["h[1] >>> ",{"fg":2}],["num                                                                                         ",{}]]},"cursor":{"x":12}}],[0.43252,{"lines":{"32":[["h[1] >>> ",{"fg":2}],["nums                                                                                        ",{}]]},"cursor":{"x":13}}],[0.354822,{"cursor":{"x":14}}],[0.142515,{"lines":{"32":[["h[1] >>> ",{"fg":2}],["nums =                                                                                      ",{}]]},"cursor":{"x":15}}],[0.07058499999999999,{"cursor":{"x":16}}],[0.359467,{"lines":{"32":[["h[1] >>> ",{"fg":2}],["nums = r                                                                                    ",{}]]},"cursor":{"x":17}}],[0.144586,{"lines":{"32":[["h[1] >>> ",{"fg":2}],["nums = ra                                                                                   ",{}]]},"cursor":{"x":18}}],[0.111365,{"lines":{"32":[["h[1] >>> ",{"fg":2}],["nums = ran                                                                                  ",{}]]},"cursor":{"x":19}}],[0.096152,{"lines":{"32":[["h[1] >>> ",{"fg":2}],["nums = rang                                                                                 ",{}]]},"cursor":{"x":20}}],[0.095544,{"lines":{"32":[["h[1] >>> ",{"fg":2}],["nums = range                                                                                ",{}]]},"cursor":{"x":21}}],[0.401793,{"lines":{"32":[["h[1] >>> ",{"fg":2}],["nums = range(                                                                               ",{}]]},"cursor":{"x":22}}],[0.07945199999999999,{"lines":{"32":[["h[1] >>> ",{"fg":2}],["nums = range()                                                                              ",{}]]},"cursor":{"x":23}}],[0.428801,{"cursor":{"x":22}}],[0.170861,{"lines":{"32":[["h[1] >>> ",{"fg":2}],["nums = range(1)                                                                             ",{}]]},"cursor":{"x":23}}],[0.178701,{"lines":{"32":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]]},"cursor":{"x":24}}],[0.726482,{"cursor":{"x":0,"y":33}}],[0.000297,{"lines":{"33":[["h[1] >>> ",{"fg":2}],["                                                                                            ",{}]]},"cursor":{"x":9}}],[3.117997,{"lines":{"33":[["h[1] >>> ",{"fg":2}],["r                                                                                           ",{}]]},"cursor":{"x":10}}],[0.360075,{"lines":{"33":[["h[1] >>> ",{"fg":2}],["rd                                                                                          ",{}]]},"cursor":{"x":11}}],[0.5041679999999999,{"lines":{"33":[["h[1] >>> ",{"fg":2}],["rdd                                                                                         ",{}]]},"cursor":{"x":12}}],[0.514916,{"cursor":{"x":13}}],[0.095748,{"lines":{"33":[["h[1] >>> ",{"fg":2}],["rdd =                                                                                       ",{}]]},"cursor":{"x":14}}],[0.117617,{"cursor":{"x":15}}],[1.151703,{"lines":{"33":[["h[1] >>> ",{"fg":2}],["rdd = s                                                                                     ",{}]]},"cursor":{"x":16}}],[0.319511,{"lines":{"33":[["h[1] >>> ",{"fg":2}],["rdd = sc                                                                                    ",{}]]},"cursor":{"x":17}}],[0.440355,{"lines":{"33":[["h[1] >>> ",{"fg":2}],["rdd = sc.                                                                                   ",{}]]},"cursor":{"x":18}}],[0.479351,{"lines":{"33":[["h[1] >>> ",{"fg":2}],["rdd = sc.p                                                                                  ",{}]]},"cursor":{"x":19}}],[0.210481,{"lines":{"33":[["h[1] >>> ",{"fg":2}],["rdd = sc.pa                                                                                 ",{}]]},"cursor":{"x":20}}],[0.407098,{"lines":{"33":[["h[1] >>> ",{"fg":2}],["rdd = sc.par                                                                                ",{}]]},"cursor":{"x":21}}],[0.240893,{"lines":{"33":[["h[1] >>> ",{"fg":2}],["rdd = sc.para                                                                               ",{}]]},"cursor":{"x":22}}],[0.295435,{"lines":{"33":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(                                                                       ",{}]]},"cursor":{"x":30}}],[1.350287,{"lines":{"33":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(n                                                                      ",{}]]},"cursor":{"x":31}}],[0.247738,{"lines":{"33":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nu                                                                     ",{}]]},"cursor":{"x":32}}],[0.216652,{"lines":{"33":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(num                                                                    ",{}]]},"cursor":{"x":33}}],[0.135636,{"lines":{"33":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums                                                                   ",{}]]},"cursor":{"x":34}}],[0.440682,{"lines":{"33":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]]},"cursor":{"x":35}}],[0.6874400000000001,{"cursor":{"x":0,"y":34}}],[0.020744,{"lines":{"34":[["h[1] >>> ",{"fg":2}],["                                                                                            ",{}]]},"cursor":{"x":9}}],[1.347025,{"lines":{"34":[["h[1] >>> ",{"fg":2}],["r                                                                                           ",{}]]},"cursor":{"x":10}}],[0.457106,{"lines":{"34":[["h[1] >>> ",{"fg":2}],["rd                                                                                          ",{}]]},"cursor":{"x":11}}],[0.151587,{"lines":{"34":[["h[1] >>> ",{"fg":2}],["rdd                                                                                         ",{}]]},"cursor":{"x":12}}],[0.271783,{"lines":{"34":[["h[1] >>> ",{"fg":2}],["rdd.                                                                                        ",{}]]},"cursor":{"x":13}}],[0.440244,{"lines":{"34":[["h[1] >>> ",{"fg":2}],["rdd.c                                                                                       ",{}]]},"cursor":{"x":14}}],[0.15135,{"lines":{"34":[["h[1] >>> ",{"fg":2}],["rdd.co                                                                                      ",{}]]},"cursor":{"x":15}}],[0.376378,{}],[1.224331,{"lines":{"35":[["rdd.cogroup(       rdd.collectAsMap(  rdd.context        rdd.countByKey(                             ",{}]],"36":[["rdd.collect(       rdd.combineByKey(  rdd.count(         rdd.countByValue(                           ",{}]],"37":[["h[1] >>> ",{"fg":2}],["rdd.co                                                                                      ",{}]]},"cursor":{"y":37}}],[0.881393,{"lines":{"37":[["h[1] >>> ",{"fg":2}],["rdd.col                                                                                     ",{}]]},"cursor":{"x":16}}],[0.430164,{"lines":{"37":[["h[1] >>> ",{"fg":2}],["rdd.cole                                                                                    ",{}]]},"cursor":{"x":17}}],[0.240335,{}],[1.76728,{"lines":{"37":[["h[1] >>> ",{"fg":2}],["rdd.col                                                                                     ",{}]]},"cursor":{"x":16}}],[0.282757,{"lines":{"37":[["h[1] >>> ",{"fg":2}],["rdd.coll                                                                                    ",{}]]},"cursor":{"x":17}}],[0.437132,{"lines":{"37":[["h[1] >>> ",{"fg":2}],["rdd.colle                                                                                   ",{}]]},"cursor":{"x":18}}],[0.210712,{"lines":{"37":[["h[1] >>> ",{"fg":2}],["rdd.collect                                                                                 ",{}]]},"cursor":{"x":20}}],[0.880178,{"lines":{"37":[["h[1] >>> ",{"fg":2}],["rdd.collect(                                                                                ",{}]]},"cursor":{"x":21}}],[0.038564,{"lines":{"37":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]]},"cursor":{"x":22}}],[0.482531,{"cursor":{"x":0,"y":38}}],[0.039025,{"lines":{"38":[["14/04/13 17:03:25 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]]},"cursor":{"y":39}}],[0.013868,{"lines":{"0":[["Python 2.7.3 |Anaconda 1.4.0 (x86_64)| (default, Feb 25 2013, 18:45:56)                              ",{}]],"1":[["[GCC 4.0.1 (Apple Inc. build 5493)] on darwin                                                        ",{}]],"2":[["Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.                               ",{}]],"3":[["14/04/13 17:02:56 INFO Slf4jEventHandler: Slf4jEventHandler started                                  ",{}]],"4":[["14/04/13 17:02:56 INFO SparkEnv: Registering BlockManagerMaster                                      ",{}]],"5":[["14/04/13 17:02:56 INFO DiskBlockManager: Created local directory at /var/folders/_g/0qtqr8zs2jx7n86sg",{}]],"6":[["xw3r_nw0000gn/T/spark-local-20140413170256-bc56                                                      ",{}]],"7":[["14/04/13 17:02:56 INFO MemoryStore: MemoryStore started with capacity 333.7 MB.                      ",{}]],"8":[["14/04/13 17:02:56 INFO ConnectionManager: Bound socket to port 63069 with id = ConnectionManagerId(19",{}]],"9":[["2.168.1.102,63069)                                                                                   ",{}]],"10":[["14/04/13 17:02:56 INFO BlockManagerMaster: Trying to register BlockManager                           ",{}]],"11":[["14/04/13 17:02:56 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 192.168.1.",{}]],"12":[["102:63069 with 333.7 MB RAM                                                                          ",{}]],"13":[["14/04/13 17:02:56 INFO BlockManagerMaster: Registered BlockManager                                   ",{}]],"14":[["14/04/13 17:02:56 INFO HttpBroadcast: Broadcast server started at http://192.168.1.102:63070         ",{}]],"15":[["14/04/13 17:02:56 INFO SparkEnv: Registering MapOutputTracker                                        ",{}]],"16":[["14/04/13 17:02:56 INFO HttpFileServer: HTTP File server directory is /var/folders/_g/0qtqr8zs2jx7n86s",{}]],"17":[["gxw3r_nw0000gn/T/spark-0f29d47d-3db6-4956-9af8-c12ef6a5f0b0                                          ",{}]],"18":[["14/04/13 17:02:57 INFO SparkUI: Started Spark Web UI at http://192.168.1.102:4040                    ",{}]],"19":[["2014-04-13 17:02:57.213 java[7240:c003] Unable to load realm info from SCDynamicStore                ",{}]],"20":[["Welcome to                                                                                           ",{}]],"21":[["      ____              __                                                                           ",{}]],"22":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"23":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"24":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"25":[["      /_/                                                                                            ",{}]],"26":[["                                                                                                     ",{}]],"27":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"28":[["Spark context avaiable as sc.                                                                        ",{}]],"29":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"30":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"31":[["h[1] >>> ",{"fg":2}],["rdd.co                                                                                      ",{}]],"32":[["rdd.cogroup(       rdd.collectAsMap(  rdd.context        rdd.countByKey(                             ",{}]],"33":[["rdd.collect(       rdd.combineByKey(  rdd.count(         rdd.countByValue(                           ",{}]],"34":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"35":[["14/04/13 17:03:25 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"36":[["14/04/13 17:03:25 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"37":[["utput partitions (allowLocal=false)                                                                  ",{}]],"38":[["14/04/13 17:03:25 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"39":[["2)                                                                                                   ",{}]],"40":[["14/04/13 17:03:25 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]]},"cursor":{"y":41}}],[0.015668,{"lines":{"0":[["14/04/13 17:02:56 INFO Slf4jEventHandler: Slf4jEventHandler started                                  ",{}]],"1":[["14/04/13 17:02:56 INFO SparkEnv: Registering BlockManagerMaster                                      ",{}]],"2":[["14/04/13 17:02:56 INFO DiskBlockManager: Created local directory at /var/folders/_g/0qtqr8zs2jx7n86sg",{}]],"3":[["xw3r_nw0000gn/T/spark-local-20140413170256-bc56                                                      ",{}]],"4":[["14/04/13 17:02:56 INFO MemoryStore: MemoryStore started with capacity 333.7 MB.                      ",{}]],"5":[["14/04/13 17:02:56 INFO ConnectionManager: Bound socket to port 63069 with id = ConnectionManagerId(19",{}]],"6":[["2.168.1.102,63069)                                                                                   ",{}]],"7":[["14/04/13 17:02:56 INFO BlockManagerMaster: Trying to register BlockManager                           ",{}]],"8":[["14/04/13 17:02:56 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 192.168.1.",{}]],"9":[["102:63069 with 333.7 MB RAM                                                                          ",{}]],"10":[["14/04/13 17:02:56 INFO BlockManagerMaster: Registered BlockManager                                   ",{}]],"11":[["14/04/13 17:02:56 INFO HttpBroadcast: Broadcast server started at http://192.168.1.102:63070         ",{}]],"12":[["14/04/13 17:02:56 INFO SparkEnv: Registering MapOutputTracker                                        ",{}]],"13":[["14/04/13 17:02:56 INFO HttpFileServer: HTTP File server directory is /var/folders/_g/0qtqr8zs2jx7n86s",{}]],"14":[["gxw3r_nw0000gn/T/spark-0f29d47d-3db6-4956-9af8-c12ef6a5f0b0                                          ",{}]],"15":[["14/04/13 17:02:57 INFO SparkUI: Started Spark Web UI at http://192.168.1.102:4040                    ",{}]],"16":[["2014-04-13 17:02:57.213 java[7240:c003] Unable to load realm info from SCDynamicStore                ",{}]],"17":[["Welcome to                                                                                           ",{}]],"18":[["      ____              __                                                                           ",{}]],"19":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"20":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"21":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"22":[["      /_/                                                                                            ",{}]],"23":[["                                                                                                     ",{}]],"24":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"25":[["Spark context avaiable as sc.                                                                        ",{}]],"26":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"27":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"28":[["h[1] >>> ",{"fg":2}],["rdd.co                                                                                      ",{}]],"29":[["rdd.cogroup(       rdd.collectAsMap(  rdd.context        rdd.countByKey(                             ",{}]],"30":[["rdd.collect(       rdd.combineByKey(  rdd.count(         rdd.countByValue(                           ",{}]],"31":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"32":[["14/04/13 17:03:25 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"33":[["14/04/13 17:03:25 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"34":[["utput partitions (allowLocal=false)                                                                  ",{}]],"35":[["14/04/13 17:03:25 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"36":[["2)                                                                                                   ",{}]],"37":[["14/04/13 17:03:25 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"38":[["14/04/13 17:03:25 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"39":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"40":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]]}}],[0.076387,{"lines":{"0":[["14/04/13 17:02:56 INFO DiskBlockManager: Created local directory at /var/folders/_g/0qtqr8zs2jx7n86sg",{}]],"1":[["xw3r_nw0000gn/T/spark-local-20140413170256-bc56                                                      ",{}]],"2":[["14/04/13 17:02:56 INFO MemoryStore: MemoryStore started with capacity 333.7 MB.                      ",{}]],"3":[["14/04/13 17:02:56 INFO ConnectionManager: Bound socket to port 63069 with id = ConnectionManagerId(19",{}]],"4":[["2.168.1.102,63069)                                                                                   ",{}]],"5":[["14/04/13 17:02:56 INFO BlockManagerMaster: Trying to register BlockManager                           ",{}]],"6":[["14/04/13 17:02:56 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 192.168.1.",{}]],"7":[["102:63069 with 333.7 MB RAM                                                                          ",{}]],"8":[["14/04/13 17:02:56 INFO BlockManagerMaster: Registered BlockManager                                   ",{}]],"9":[["14/04/13 17:02:56 INFO HttpBroadcast: Broadcast server started at http://192.168.1.102:63070         ",{}]],"10":[["14/04/13 17:02:56 INFO SparkEnv: Registering MapOutputTracker                                        ",{}]],"11":[["14/04/13 17:02:56 INFO HttpFileServer: HTTP File server directory is /var/folders/_g/0qtqr8zs2jx7n86s",{}]],"12":[["gxw3r_nw0000gn/T/spark-0f29d47d-3db6-4956-9af8-c12ef6a5f0b0                                          ",{}]],"13":[["14/04/13 17:02:57 INFO SparkUI: Started Spark Web UI at http://192.168.1.102:4040                    ",{}]],"14":[["2014-04-13 17:02:57.213 java[7240:c003] Unable to load realm info from SCDynamicStore                ",{}]],"15":[["Welcome to                                                                                           ",{}]],"16":[["      ____              __                                                                           ",{}]],"17":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"18":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"19":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"20":[["      /_/                                                                                            ",{}]],"21":[["                                                                                                     ",{}]],"22":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"23":[["Spark context avaiable as sc.                                                                        ",{}]],"24":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"25":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"26":[["h[1] >>> ",{"fg":2}],["rdd.co                                                                                      ",{}]],"27":[["rdd.cogroup(       rdd.collectAsMap(  rdd.context        rdd.countByKey(                             ",{}]],"28":[["rdd.collect(       rdd.combineByKey(  rdd.count(         rdd.countByValue(                           ",{}]],"29":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"30":[["14/04/13 17:03:25 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"31":[["14/04/13 17:03:25 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"32":[["utput partitions (allowLocal=false)                                                                  ",{}]],"33":[["14/04/13 17:03:25 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"34":[["2)                                                                                                   ",{}]],"35":[["14/04/13 17:03:25 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"36":[["14/04/13 17:03:25 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"37":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"38":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"39":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"40":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]]}}],[0.022159,{"lines":{"0":[["xw3r_nw0000gn/T/spark-local-20140413170256-bc56                                                      ",{}]],"1":[["14/04/13 17:02:56 INFO MemoryStore: MemoryStore started with capacity 333.7 MB.                      ",{}]],"2":[["14/04/13 17:02:56 INFO ConnectionManager: Bound socket to port 63069 with id = ConnectionManagerId(19",{}]],"3":[["2.168.1.102,63069)                                                                                   ",{}]],"4":[["14/04/13 17:02:56 INFO BlockManagerMaster: Trying to register BlockManager                           ",{}]],"5":[["14/04/13 17:02:56 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 192.168.1.",{}]],"6":[["102:63069 with 333.7 MB RAM                                                                          ",{}]],"7":[["14/04/13 17:02:56 INFO BlockManagerMaster: Registered BlockManager                                   ",{}]],"8":[["14/04/13 17:02:56 INFO HttpBroadcast: Broadcast server started at http://192.168.1.102:63070         ",{}]],"9":[["14/04/13 17:02:56 INFO SparkEnv: Registering MapOutputTracker                                        ",{}]],"10":[["14/04/13 17:02:56 INFO HttpFileServer: HTTP File server directory is /var/folders/_g/0qtqr8zs2jx7n86s",{}]],"11":[["gxw3r_nw0000gn/T/spark-0f29d47d-3db6-4956-9af8-c12ef6a5f0b0                                          ",{}]],"12":[["14/04/13 17:02:57 INFO SparkUI: Started Spark Web UI at http://192.168.1.102:4040                    ",{}]],"13":[["2014-04-13 17:02:57.213 java[7240:c003] Unable to load realm info from SCDynamicStore                ",{}]],"14":[["Welcome to                                                                                           ",{}]],"15":[["      ____              __                                                                           ",{}]],"16":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"17":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"18":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"19":[["      /_/                                                                                            ",{}]],"20":[["                                                                                                     ",{}]],"21":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"22":[["Spark context avaiable as sc.                                                                        ",{}]],"23":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"24":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"25":[["h[1] >>> ",{"fg":2}],["rdd.co                                                                                      ",{}]],"26":[["rdd.cogroup(       rdd.collectAsMap(  rdd.context        rdd.countByKey(                             ",{}]],"27":[["rdd.collect(       rdd.combineByKey(  rdd.count(         rdd.countByValue(                           ",{}]],"28":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"29":[["14/04/13 17:03:25 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"30":[["14/04/13 17:03:25 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"31":[["utput partitions (allowLocal=false)                                                                  ",{}]],"32":[["14/04/13 17:03:25 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"33":[["2)                                                                                                   ",{}]],"34":[["14/04/13 17:03:25 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"35":[["14/04/13 17:03:25 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"36":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"37":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"38":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"39":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]],"40":[["14/04/13 17:03:25 INFO LocalTaskSetManager: Size of task 0 is 1215 bytes                             ",{}]]}}],[0.010933,{"lines":{"0":[["14/04/13 17:02:56 INFO MemoryStore: MemoryStore started with capacity 333.7 MB.                      ",{}]],"1":[["14/04/13 17:02:56 INFO ConnectionManager: Bound socket to port 63069 with id = ConnectionManagerId(19",{}]],"2":[["2.168.1.102,63069)                                                                                   ",{}]],"3":[["14/04/13 17:02:56 INFO BlockManagerMaster: Trying to register BlockManager                           ",{}]],"4":[["14/04/13 17:02:56 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 192.168.1.",{}]],"5":[["102:63069 with 333.7 MB RAM                                                                          ",{}]],"6":[["14/04/13 17:02:56 INFO BlockManagerMaster: Registered BlockManager                                   ",{}]],"7":[["14/04/13 17:02:56 INFO HttpBroadcast: Broadcast server started at http://192.168.1.102:63070         ",{}]],"8":[["14/04/13 17:02:56 INFO SparkEnv: Registering MapOutputTracker                                        ",{}]],"9":[["14/04/13 17:02:56 INFO HttpFileServer: HTTP File server directory is /var/folders/_g/0qtqr8zs2jx7n86s",{}]],"10":[["gxw3r_nw0000gn/T/spark-0f29d47d-3db6-4956-9af8-c12ef6a5f0b0                                          ",{}]],"11":[["14/04/13 17:02:57 INFO SparkUI: Started Spark Web UI at http://192.168.1.102:4040                    ",{}]],"12":[["2014-04-13 17:02:57.213 java[7240:c003] Unable to load realm info from SCDynamicStore                ",{}]],"13":[["Welcome to                                                                                           ",{}]],"14":[["      ____              __                                                                           ",{}]],"15":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"16":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"17":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"18":[["      /_/                                                                                            ",{}]],"19":[["                                                                                                     ",{}]],"20":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"21":[["Spark context avaiable as sc.                                                                        ",{}]],"22":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"23":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"24":[["h[1] >>> ",{"fg":2}],["rdd.co                                                                                      ",{}]],"25":[["rdd.cogroup(       rdd.collectAsMap(  rdd.context        rdd.countByKey(                             ",{}]],"26":[["rdd.collect(       rdd.combineByKey(  rdd.count(         rdd.countByValue(                           ",{}]],"27":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"28":[["14/04/13 17:03:25 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"29":[["14/04/13 17:03:25 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"30":[["utput partitions (allowLocal=false)                                                                  ",{}]],"31":[["14/04/13 17:03:25 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"32":[["2)                                                                                                   ",{}]],"33":[["14/04/13 17:03:25 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"34":[["14/04/13 17:03:25 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"35":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"36":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"37":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"38":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]],"39":[["14/04/13 17:03:25 INFO LocalTaskSetManager: Size of task 0 is 1215 bytes                             ",{}]],"40":[["14/04/13 17:03:25 INFO Executor: Running task ID 0                                                   ",{}]]}}],[0.038205,{"lines":{"0":[["14/04/13 17:02:56 INFO ConnectionManager: Bound socket to port 63069 with id = ConnectionManagerId(19",{}]],"1":[["2.168.1.102,63069)                                                                                   ",{}]],"2":[["14/04/13 17:02:56 INFO BlockManagerMaster: Trying to register BlockManager                           ",{}]],"3":[["14/04/13 17:02:56 INFO BlockManagerMasterActor$BlockManagerInfo: Registering block manager 192.168.1.",{}]],"4":[["102:63069 with 333.7 MB RAM                                                                          ",{}]],"5":[["14/04/13 17:02:56 INFO BlockManagerMaster: Registered BlockManager                                   ",{}]],"6":[["14/04/13 17:02:56 INFO HttpBroadcast: Broadcast server started at http://192.168.1.102:63070         ",{}]],"7":[["14/04/13 17:02:56 INFO SparkEnv: Registering MapOutputTracker                                        ",{}]],"8":[["14/04/13 17:02:56 INFO HttpFileServer: HTTP File server directory is /var/folders/_g/0qtqr8zs2jx7n86s",{}]],"9":[["gxw3r_nw0000gn/T/spark-0f29d47d-3db6-4956-9af8-c12ef6a5f0b0                                          ",{}]],"10":[["14/04/13 17:02:57 INFO SparkUI: Started Spark Web UI at http://192.168.1.102:4040                    ",{}]],"11":[["2014-04-13 17:02:57.213 java[7240:c003] Unable to load realm info from SCDynamicStore                ",{}]],"12":[["Welcome to                                                                                           ",{}]],"13":[["      ____              __                                                                           ",{}]],"14":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"15":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"16":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"17":[["      /_/                                                                                            ",{}]],"18":[["                                                                                                     ",{}]],"19":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"20":[["Spark context avaiable as sc.                                                                        ",{}]],"21":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"22":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"23":[["h[1] >>> ",{"fg":2}],["rdd.co                                                                                      ",{}]],"24":[["rdd.cogroup(       rdd.collectAsMap(  rdd.context        rdd.countByKey(                             ",{}]],"25":[["rdd.collect(       rdd.combineByKey(  rdd.count(         rdd.countByValue(                           ",{}]],"26":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"27":[["14/04/13 17:03:25 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"28":[["14/04/13 17:03:25 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"29":[["utput partitions (allowLocal=false)                                                                  ",{}]],"30":[["14/04/13 17:03:25 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"31":[["2)                                                                                                   ",{}]],"32":[["14/04/13 17:03:25 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"33":[["14/04/13 17:03:25 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"34":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"35":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"36":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"37":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]],"38":[["14/04/13 17:03:25 INFO LocalTaskSetManager: Size of task 0 is 1215 bytes                             ",{}]],"39":[["14/04/13 17:03:25 INFO Executor: Running task ID 0                                                   ",{}]],"40":[["14/04/13 17:03:26 INFO Executor: Serialized size of result for 0 is 517                              ",{}]]}}],[0.010309,{"lines":{"0":[["14/04/13 17:02:56 INFO HttpFileServer: HTTP File server directory is /var/folders/_g/0qtqr8zs2jx7n86s",{}]],"1":[["gxw3r_nw0000gn/T/spark-0f29d47d-3db6-4956-9af8-c12ef6a5f0b0                                          ",{}]],"2":[["14/04/13 17:02:57 INFO SparkUI: Started Spark Web UI at http://192.168.1.102:4040                    ",{}]],"3":[["2014-04-13 17:02:57.213 java[7240:c003] Unable to load realm info from SCDynamicStore                ",{}]],"4":[["Welcome to                                                                                           ",{}]],"5":[["      ____              __                                                                           ",{}]],"6":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"7":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"8":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"9":[["      /_/                                                                                            ",{}]],"10":[["                                                                                                     ",{}]],"11":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"12":[["Spark context avaiable as sc.                                                                        ",{}]],"13":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"14":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"15":[["h[1] >>> ",{"fg":2}],["rdd.co                                                                                      ",{}]],"16":[["rdd.cogroup(       rdd.collectAsMap(  rdd.context        rdd.countByKey(                             ",{}]],"17":[["rdd.collect(       rdd.combineByKey(  rdd.count(         rdd.countByValue(                           ",{}]],"18":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"19":[["14/04/13 17:03:25 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"20":[["14/04/13 17:03:25 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"21":[["utput partitions (allowLocal=false)                                                                  ",{}]],"22":[["14/04/13 17:03:25 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"23":[["2)                                                                                                   ",{}]],"24":[["14/04/13 17:03:25 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"25":[["14/04/13 17:03:25 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"26":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"27":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"28":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"29":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]],"30":[["14/04/13 17:03:25 INFO LocalTaskSetManager: Size of task 0 is 1215 bytes                             ",{}]],"31":[["14/04/13 17:03:25 INFO Executor: Running task ID 0                                                   ",{}]],"32":[["14/04/13 17:03:26 INFO Executor: Serialized size of result for 0 is 517                              ",{}]],"33":[["14/04/13 17:03:26 INFO Executor: Sending result for 0 directly to driver                             ",{}]],"34":[["14/04/13 17:03:26 INFO DAGScheduler: Completed ResultTask(0, 0)                                      ",{}]],"35":[["14/04/13 17:03:26 INFO LocalScheduler: Remove TaskSet 0.0 from pool                                  ",{}]],"36":[["14/04/13 17:03:26 INFO DAGScheduler: Stage 0 (collect at NativeMethodAccessorImpl.java:-2) finished i",{}]],"37":[["n 0.072 s                                                                                            ",{}]],"38":[["14/04/13 17:03:26 INFO SparkContext: Job finished: collect at NativeMethodAccessorImpl.java:-2, took ",{}]],"39":[["0.186915 s                                                                                           ",{}]],"40":[["14/04/13 17:03:26 INFO Executor: Finished task ID 0                                                  ",{}]]}}],[0.009851,{"lines":{"0":[["gxw3r_nw0000gn/T/spark-0f29d47d-3db6-4956-9af8-c12ef6a5f0b0                                          ",{}]],"1":[["14/04/13 17:02:57 INFO SparkUI: Started Spark Web UI at http://192.168.1.102:4040                    ",{}]],"2":[["2014-04-13 17:02:57.213 java[7240:c003] Unable to load realm info from SCDynamicStore                ",{}]],"3":[["Welcome to                                                                                           ",{}]],"4":[["      ____              __                                                                           ",{}]],"5":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"6":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"7":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"8":[["      /_/                                                                                            ",{}]],"9":[["                                                                                                     ",{}]],"10":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"11":[["Spark context avaiable as sc.                                                                        ",{}]],"12":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"13":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"14":[["h[1] >>> ",{"fg":2}],["rdd.co                                                                                      ",{}]],"15":[["rdd.cogroup(       rdd.collectAsMap(  rdd.context        rdd.countByKey(                             ",{}]],"16":[["rdd.collect(       rdd.combineByKey(  rdd.count(         rdd.countByValue(                           ",{}]],"17":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"18":[["14/04/13 17:03:25 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"19":[["14/04/13 17:03:25 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"20":[["utput partitions (allowLocal=false)                                                                  ",{}]],"21":[["14/04/13 17:03:25 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"22":[["2)                                                                                                   ",{}]],"23":[["14/04/13 17:03:25 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"24":[["14/04/13 17:03:25 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"25":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"26":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"27":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"28":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]],"29":[["14/04/13 17:03:25 INFO LocalTaskSetManager: Size of task 0 is 1215 bytes                             ",{}]],"30":[["14/04/13 17:03:25 INFO Executor: Running task ID 0                                                   ",{}]],"31":[["14/04/13 17:03:26 INFO Executor: Serialized size of result for 0 is 517                              ",{}]],"32":[["14/04/13 17:03:26 INFO Executor: Sending result for 0 directly to driver                             ",{}]],"33":[["14/04/13 17:03:26 INFO DAGScheduler: Completed ResultTask(0, 0)                                      ",{}]],"34":[["14/04/13 17:03:26 INFO LocalScheduler: Remove TaskSet 0.0 from pool                                  ",{}]],"35":[["14/04/13 17:03:26 INFO DAGScheduler: Stage 0 (collect at NativeMethodAccessorImpl.java:-2) finished i",{}]],"36":[["n 0.072 s                                                                                            ",{}]],"37":[["14/04/13 17:03:26 INFO SparkContext: Job finished: collect at NativeMethodAccessorImpl.java:-2, took ",{}]],"38":[["0.186915 s                                                                                           ",{}]],"39":[["14/04/13 17:03:26 INFO Executor: Finished task ID 0                                                  ",{}]],"40":[["[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]                                                                       ",{}]],"41":[["h[2] >>> ",{"fg":2}],["                                                                                            ",{}]]},"cursor":{"x":9}}],[2.30117,{"lines":{"41":[["h[2] >>> ",{"fg":2}],["^C                                                                                          ",{}]]},"cursor":{"x":11}}],[0.000228,{"lines":{"0":[["2014-04-13 17:02:57.213 java[7240:c003] Unable to load realm info from SCDynamicStore                ",{}]],"1":[["Welcome to                                                                                           ",{}]],"2":[["      ____              __                                                                           ",{}]],"3":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"4":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"5":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"6":[["      /_/                                                                                            ",{}]],"7":[["                                                                                                     ",{}]],"8":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"9":[["Spark context avaiable as sc.                                                                        ",{}]],"10":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"11":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"12":[["h[1] >>> ",{"fg":2}],["rdd.co                                                                                      ",{}]],"13":[["rdd.cogroup(       rdd.collectAsMap(  rdd.context        rdd.countByKey(                             ",{}]],"14":[["rdd.collect(       rdd.combineByKey(  rdd.count(         rdd.countByValue(                           ",{}]],"15":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"16":[["14/04/13 17:03:25 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"17":[["14/04/13 17:03:25 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"18":[["utput partitions (allowLocal=false)                                                                  ",{}]],"19":[["14/04/13 17:03:25 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"20":[["2)                                                                                                   ",{}]],"21":[["14/04/13 17:03:25 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"22":[["14/04/13 17:03:25 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"23":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"24":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"25":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"26":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]],"27":[["14/04/13 17:03:25 INFO LocalTaskSetManager: Size of task 0 is 1215 bytes                             ",{}]],"28":[["14/04/13 17:03:25 INFO Executor: Running task ID 0                                                   ",{}]],"29":[["14/04/13 17:03:26 INFO Executor: Serialized size of result for 0 is 517                              ",{}]],"30":[["14/04/13 17:03:26 INFO Executor: Sending result for 0 directly to driver                             ",{}]],"31":[["14/04/13 17:03:26 INFO DAGScheduler: Completed ResultTask(0, 0)                                      ",{}]],"32":[["14/04/13 17:03:26 INFO LocalScheduler: Remove TaskSet 0.0 from pool                                  ",{}]],"33":[["14/04/13 17:03:26 INFO DAGScheduler: Stage 0 (collect at NativeMethodAccessorImpl.java:-2) finished i",{}]],"34":[["n 0.072 s                                                                                            ",{}]],"35":[["14/04/13 17:03:26 INFO SparkContext: Job finished: collect at NativeMethodAccessorImpl.java:-2, took ",{}]],"36":[["0.186915 s                                                                                           ",{}]],"37":[["14/04/13 17:03:26 INFO Executor: Finished task ID 0                                                  ",{}]],"38":[["[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]                                                                       ",{}]],"39":[["h[2] >>> ",{"fg":2}],["^C                                                                                          ",{}]],"40":[["KeyboardInterrupt                                                                                    ",{}]],"41":[["h[2] >>> ",{"fg":2}],["                                                                                            ",{}]]},"cursor":{"x":9}}],[0.439869,{"lines":{"0":[["Welcome to                                                                                           ",{}]],"1":[["      ____              __                                                                           ",{}]],"2":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"3":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"4":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"5":[["      /_/                                                                                            ",{}]],"6":[["                                                                                                     ",{}]],"7":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"8":[["Spark context avaiable as sc.                                                                        ",{}]],"9":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"10":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"11":[["h[1] >>> ",{"fg":2}],["rdd.co                                                                                      ",{}]],"12":[["rdd.cogroup(       rdd.collectAsMap(  rdd.context        rdd.countByKey(                             ",{}]],"13":[["rdd.collect(       rdd.combineByKey(  rdd.count(         rdd.countByValue(                           ",{}]],"14":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"15":[["14/04/13 17:03:25 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"16":[["14/04/13 17:03:25 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"17":[["utput partitions (allowLocal=false)                                                                  ",{}]],"18":[["14/04/13 17:03:25 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"19":[["2)                                                                                                   ",{}]],"20":[["14/04/13 17:03:25 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"21":[["14/04/13 17:03:25 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"22":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"23":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"24":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"25":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]],"26":[["14/04/13 17:03:25 INFO LocalTaskSetManager: Size of task 0 is 1215 bytes                             ",{}]],"27":[["14/04/13 17:03:25 INFO Executor: Running task ID 0                                                   ",{}]],"28":[["14/04/13 17:03:26 INFO Executor: Serialized size of result for 0 is 517                              ",{}]],"29":[["14/04/13 17:03:26 INFO Executor: Sending result for 0 directly to driver                             ",{}]],"30":[["14/04/13 17:03:26 INFO DAGScheduler: Completed ResultTask(0, 0)                                      ",{}]],"31":[["14/04/13 17:03:26 INFO LocalScheduler: Remove TaskSet 0.0 from pool                                  ",{}]],"32":[["14/04/13 17:03:26 INFO DAGScheduler: Stage 0 (collect at NativeMethodAccessorImpl.java:-2) finished i",{}]],"33":[["n 0.072 s                                                                                            ",{}]],"34":[["14/04/13 17:03:26 INFO SparkContext: Job finished: collect at NativeMethodAccessorImpl.java:-2, took ",{}]],"35":[["0.186915 s                                                                                           ",{}]],"36":[["14/04/13 17:03:26 INFO Executor: Finished task ID 0                                                  ",{}]],"37":[["[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]                                                                       ",{}]],"38":[["h[2] >>> ",{"fg":2}],["^C                                                                                          ",{}]],"39":[["KeyboardInterrupt                                                                                    ",{}]],"40":[["h[2] >>> ",{"fg":2}],["                                                                                            ",{}]],"41":[["                                                                                                     ",{}]]},"cursor":{"x":0}}],[0.011655,{}],[0.030912,{"lines":{"0":[["      ____              __                                                                           ",{}]],"1":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"2":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"3":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"4":[["      /_/                                                                                            ",{}]],"5":[["                                                                                                     ",{}]],"6":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"7":[["Spark context avaiable as sc.                                                                        ",{}]],"8":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"9":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"10":[["h[1] >>> ",{"fg":2}],["rdd.co                                                                                      ",{}]],"11":[["rdd.cogroup(       rdd.collectAsMap(  rdd.context        rdd.countByKey(                             ",{}]],"12":[["rdd.collect(       rdd.combineByKey(  rdd.count(         rdd.countByValue(                           ",{}]],"13":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"14":[["14/04/13 17:03:25 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"15":[["14/04/13 17:03:25 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"16":[["utput partitions (allowLocal=false)                                                                  ",{}]],"17":[["14/04/13 17:03:25 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"18":[["2)                                                                                                   ",{}]],"19":[["14/04/13 17:03:25 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"20":[["14/04/13 17:03:25 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"21":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"22":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"23":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"24":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]],"25":[["14/04/13 17:03:25 INFO LocalTaskSetManager: Size of task 0 is 1215 bytes                             ",{}]],"26":[["14/04/13 17:03:25 INFO Executor: Running task ID 0                                                   ",{}]],"27":[["14/04/13 17:03:26 INFO Executor: Serialized size of result for 0 is 517                              ",{}]],"28":[["14/04/13 17:03:26 INFO Executor: Sending result for 0 directly to driver                             ",{}]],"29":[["14/04/13 17:03:26 INFO DAGScheduler: Completed ResultTask(0, 0)                                      ",{}]],"30":[["14/04/13 17:03:26 INFO LocalScheduler: Remove TaskSet 0.0 from pool                                  ",{}]],"31":[["14/04/13 17:03:26 INFO DAGScheduler: Stage 0 (collect at NativeMethodAccessorImpl.java:-2) finished i",{}]],"32":[["n 0.072 s                                                                                            ",{}]],"33":[["14/04/13 17:03:26 INFO SparkContext: Job finished: collect at NativeMethodAccessorImpl.java:-2, took ",{}]],"34":[["0.186915 s                                                                                           ",{}]],"35":[["14/04/13 17:03:26 INFO Executor: Finished task ID 0                                                  ",{}]],"36":[["[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]                                                                       ",{}]],"37":[["h[2] >>> ",{"fg":2}],["^C                                                                                          ",{}]],"38":[["KeyboardInterrupt                                                                                    ",{}]],"39":[["h[2] >>> ",{"fg":2}],["                                                                                            ",{}]],"40":[["╭─",{}],["zzl",{"fg":40}],[" ",{}],["at",{"fg":239}],[" ",{}],["zhus-MacBook-Air-3",{"fg":33}],[" ",{}],["in",{"fg":239}],[" ",{}],["~/projects/scala",{"fg":226,"bold":true}],[" ",{}],["using",{"fg":239}],[" ‹›",{"fg":243}],["                                             ",{}]],"41":[["╰─○                                                                                                  ",{}]]},"cursor":{"x":4}}],[0.765394,{"lines":{"41":[["╰─○ e                                                                                                ",{}]]},"cursor":{"x":5}}],[0.453643,{"lines":{"41":[["╰─○ ex                                                                                               ",{}]]},"cursor":{"x":6}}],[0.183408,{"lines":{"41":[["╰─○ exi                                                                                              ",{}]]},"cursor":{"x":7}}],[0.157156,{"lines":{"41":[["╰─○ exit                                                                                             ",{}]]},"cursor":{"x":8}}],[0.192671,{"lines":{"0":[["     / __/__  ___ _____/ /__                                                                         ",{}]],"1":[["    _\\ \\/ _ \\/ _ `/ __/  '_/                                                                         ",{}]],"2":[["   /__ / .__/\\_,_/_/ /_/\\_\\   version 0.8.1                                                          ",{}]],"3":[["      /_/                                                                                            ",{}]],"4":[["                                                                                                     ",{}]],"5":[["Using Python version 2.7.3 (default, Feb 25 2013 18:45:56)                                           ",{}]],"6":[["Spark context avaiable as sc.                                                                        ",{}]],"7":[["h[1] >>> ",{"fg":2}],["nums = range(10)                                                                            ",{}]],"8":[["h[1] >>> ",{"fg":2}],["rdd = sc.parallelize(nums)                                                                  ",{}]],"9":[["h[1] >>> ",{"fg":2}],["rdd.co                                                                                      ",{}]],"10":[["rdd.cogroup(       rdd.collectAsMap(  rdd.context        rdd.countByKey(                             ",{}]],"11":[["rdd.collect(       rdd.combineByKey(  rdd.count(         rdd.countByValue(                           ",{}]],"12":[["h[1] >>> ",{"fg":2}],["rdd.collect()                                                                               ",{}]],"13":[["14/04/13 17:03:25 INFO SparkContext: Starting job: collect at NativeMethodAccessorImpl.java:-2       ",{}]],"14":[["14/04/13 17:03:25 INFO DAGScheduler: Got job 0 (collect at NativeMethodAccessorImpl.java:-2) with 1 o",{}]],"15":[["utput partitions (allowLocal=false)                                                                  ",{}]],"16":[["14/04/13 17:03:25 INFO DAGScheduler: Final stage: Stage 0 (collect at NativeMethodAccessorImpl.java:-",{}]],"17":[["2)                                                                                                   ",{}]],"18":[["14/04/13 17:03:25 INFO DAGScheduler: Parents of final stage: List()                                  ",{}]],"19":[["14/04/13 17:03:25 INFO DAGScheduler: Missing parents: List()                                         ",{}]],"20":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting Stage 0 (ParallelCollectionRDD[0] at parallelize at P",{}]],"21":[["ythonRDD.scala:270), which has no missing parents                                                    ",{}]],"22":[["14/04/13 17:03:25 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (ParallelCollectionRDD[0",{}]],"23":[["] at parallelize at PythonRDD.scala:270)                                                             ",{}]],"24":[["14/04/13 17:03:25 INFO LocalTaskSetManager: Size of task 0 is 1215 bytes                             ",{}]],"25":[["14/04/13 17:03:25 INFO Executor: Running task ID 0                                                   ",{}]],"26":[["14/04/13 17:03:26 INFO Executor: Serialized size of result for 0 is 517                              ",{}]],"27":[["14/04/13 17:03:26 INFO Executor: Sending result for 0 directly to driver                             ",{}]],"28":[["14/04/13 17:03:26 INFO DAGScheduler: Completed ResultTask(0, 0)                                      ",{}]],"29":[["14/04/13 17:03:26 INFO LocalScheduler: Remove TaskSet 0.0 from pool                                  ",{}]],"30":[["14/04/13 17:03:26 INFO DAGScheduler: Stage 0 (collect at NativeMethodAccessorImpl.java:-2) finished i",{}]],"31":[["n 0.072 s                                                                                            ",{}]],"32":[["14/04/13 17:03:26 INFO SparkContext: Job finished: collect at NativeMethodAccessorImpl.java:-2, took ",{}]],"33":[["0.186915 s                                                                                           ",{}]],"34":[["14/04/13 17:03:26 INFO Executor: Finished task ID 0                                                  ",{}]],"35":[["[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]                                                                       ",{}]],"36":[["h[2] >>> ",{"fg":2}],["^C                                                                                          ",{}]],"37":[["KeyboardInterrupt                                                                                    ",{}]],"38":[["h[2] >>> ",{"fg":2}],["                                                                                            ",{}]],"39":[["╭─",{}],["zzl",{"fg":40}],[" ",{}],["at",{"fg":239}],[" ",{}],["zhus-MacBook-Air-3",{"fg":33}],[" ",{}],["in",{"fg":239}],[" ",{}],["~/projects/scala",{"fg":226,"bold":true}],[" ",{}],["using",{"fg":239}],[" ‹›",{"fg":243}],["                                             ",{}]],"40":[["╰─○ exit                                                                                             ",{}]],"41":[["                                                                                                     ",{}]]},"cursor":{"x":0}}],[0.000666,{}]]